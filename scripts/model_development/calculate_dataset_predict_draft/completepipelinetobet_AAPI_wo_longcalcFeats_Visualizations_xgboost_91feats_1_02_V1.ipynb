{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Import-via-Application-Programming-Interfaces\" data-toc-modified-id=\"Data-Import-via-Application-Programming-Interfaces-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Import via Application Programming Interfaces</a></span><ul class=\"toc-item\"><li><span><a href=\"#Blockchain-Data-from-Glassnode\" data-toc-modified-id=\"Blockchain-Data-from-Glassnode-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Blockchain Data from Glassnode</a></span><ul class=\"toc-item\"><li><span><a href=\"#Market-Metrics-from-Glassnode\" data-toc-modified-id=\"Market-Metrics-from-Glassnode-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Market Metrics from Glassnode</a></span></li><li><span><a href=\"#Mining-Metrics-from-Glassnode\" data-toc-modified-id=\"Mining-Metrics-from-Glassnode-1.1.1.2\"><span class=\"toc-item-num\">1.1.1.2&nbsp;&nbsp;</span>Mining Metrics from Glassnode</a></span></li><li><span><a href=\"#Blocks-Metrics-from-Glassnode\" data-toc-modified-id=\"Blocks-Metrics-from-Glassnode-1.1.1.3\"><span class=\"toc-item-num\">1.1.1.3&nbsp;&nbsp;</span>Blocks Metrics from Glassnode</a></span></li><li><span><a href=\"#Distribution-Metrics-from-Glassnode\" data-toc-modified-id=\"Distribution-Metrics-from-Glassnode-1.1.1.4\"><span class=\"toc-item-num\">1.1.1.4&nbsp;&nbsp;</span>Distribution Metrics from Glassnode</a></span></li><li><span><a href=\"#Fee-Metrics-from-Glassnode\" data-toc-modified-id=\"Fee-Metrics-from-Glassnode-1.1.1.5\"><span class=\"toc-item-num\">1.1.1.5&nbsp;&nbsp;</span>Fee Metrics from Glassnode</a></span></li><li><span><a href=\"#UTXO-Metrics-from-Glassnode\" data-toc-modified-id=\"UTXO-Metrics-from-Glassnode-1.1.1.6\"><span class=\"toc-item-num\">1.1.1.6&nbsp;&nbsp;</span>UTXO Metrics from Glassnode</a></span></li><li><span><a href=\"#Supply-Metrics-from-Glassnode\" data-toc-modified-id=\"Supply-Metrics-from-Glassnode-1.1.1.7\"><span class=\"toc-item-num\">1.1.1.7&nbsp;&nbsp;</span>Supply Metrics from Glassnode</a></span></li><li><span><a href=\"#Transaction-Metrics-from-Glassnode\" data-toc-modified-id=\"Transaction-Metrics-from-Glassnode-1.1.1.8\"><span class=\"toc-item-num\">1.1.1.8&nbsp;&nbsp;</span>Transaction Metrics from Glassnode</a></span></li><li><span><a href=\"#Exchange-Metrics-from-Glassnode---1-Month-Lag\" data-toc-modified-id=\"Exchange-Metrics-from-Glassnode---1-Month-Lag-1.1.1.9\"><span class=\"toc-item-num\">1.1.1.9&nbsp;&nbsp;</span>Exchange Metrics from Glassnode - 1 Month Lag</a></span></li><li><span><a href=\"#Indicator-I-Metrics-from-Glassnode\" data-toc-modified-id=\"Indicator-I-Metrics-from-Glassnode-1.1.1.10\"><span class=\"toc-item-num\">1.1.1.10&nbsp;&nbsp;</span>Indicator-I Metrics from Glassnode</a></span></li><li><span><a href=\"#Indicator-2-Metrics-from-Glassnode\" data-toc-modified-id=\"Indicator-2-Metrics-from-Glassnode-1.1.1.11\"><span class=\"toc-item-num\">1.1.1.11&nbsp;&nbsp;</span>Indicator-2 Metrics from Glassnode</a></span></li><li><span><a href=\"#Indicator-3-Metrics-from-Glassnode\" data-toc-modified-id=\"Indicator-3-Metrics-from-Glassnode-1.1.1.12\"><span class=\"toc-item-num\">1.1.1.12&nbsp;&nbsp;</span>Indicator-3 Metrics from Glassnode</a></span></li><li><span><a href=\"#Addresses-Metrics-from-Glassnode\" data-toc-modified-id=\"Addresses-Metrics-from-Glassnode-1.1.1.13\"><span class=\"toc-item-num\">1.1.1.13&nbsp;&nbsp;</span>Addresses Metrics from Glassnode</a></span></li><li><span><a href=\"#Futures-Metrics-from-Glassnode---Excluded-as-Data-Available-Only-Since-Feb-2020\" data-toc-modified-id=\"Futures-Metrics-from-Glassnode---Excluded-as-Data-Available-Only-Since-Feb-2020-1.1.1.14\"><span class=\"toc-item-num\">1.1.1.14&nbsp;&nbsp;</span>Futures Metrics from Glassnode - Excluded as Data Available Only Since Feb 2020</a></span><ul class=\"toc-item\"><li><span><a href=\"#List-of-Futures-Features\" data-toc-modified-id=\"List-of-Futures-Features-1.1.1.14.1\"><span class=\"toc-item-num\">1.1.1.14.1&nbsp;&nbsp;</span>List of Futures Features</a></span></li><li><span><a href=\"#Data-Frame-of-Future-features\" data-toc-modified-id=\"Data-Frame-of-Future-features-1.1.1.14.2\"><span class=\"toc-item-num\">1.1.1.14.2&nbsp;&nbsp;</span>Data Frame of Future features</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#File-Import-and-Data-Tidying\" data-toc-modified-id=\"File-Import-and-Data-Tidying-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>File Import and Data Tidying</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Blockchain-CSV-Files\" data-toc-modified-id=\"Import-Blockchain-CSV-Files-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Import Blockchain CSV-Files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Market-Metric-CSV-Files\" data-toc-modified-id=\"Market-Metric-CSV-Files-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Market Metric CSV-Files</a></span></li><li><span><a href=\"#Mining-Metric-CSV-Files\" data-toc-modified-id=\"Mining-Metric-CSV-Files-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Mining Metric CSV-Files</a></span></li><li><span><a href=\"#Blocks-Metric-CSV-Files\" data-toc-modified-id=\"Blocks-Metric-CSV-Files-1.2.1.3\"><span class=\"toc-item-num\">1.2.1.3&nbsp;&nbsp;</span>Blocks Metric CSV-Files</a></span></li><li><span><a href=\"#Distribution-Metric-CSV-Files\" data-toc-modified-id=\"Distribution-Metric-CSV-Files-1.2.1.4\"><span class=\"toc-item-num\">1.2.1.4&nbsp;&nbsp;</span>Distribution Metric CSV-Files</a></span></li><li><span><a href=\"#Fee-Metric-CSV-Files\" data-toc-modified-id=\"Fee-Metric-CSV-Files-1.2.1.5\"><span class=\"toc-item-num\">1.2.1.5&nbsp;&nbsp;</span>Fee Metric CSV-Files</a></span></li><li><span><a href=\"#UTXO-Metric-CSV-Files\" data-toc-modified-id=\"UTXO-Metric-CSV-Files-1.2.1.6\"><span class=\"toc-item-num\">1.2.1.6&nbsp;&nbsp;</span>UTXO Metric CSV-Files</a></span></li><li><span><a href=\"#Supply-Metric-CSV-Files\" data-toc-modified-id=\"Supply-Metric-CSV-Files-1.2.1.7\"><span class=\"toc-item-num\">1.2.1.7&nbsp;&nbsp;</span>Supply Metric CSV-Files</a></span></li><li><span><a href=\"#Transaction-Metric-CSV-Files\" data-toc-modified-id=\"Transaction-Metric-CSV-Files-1.2.1.8\"><span class=\"toc-item-num\">1.2.1.8&nbsp;&nbsp;</span>Transaction Metric CSV-Files</a></span></li><li><span><a href=\"#Exchange-Metrics-CSV-Files\" data-toc-modified-id=\"Exchange-Metrics-CSV-Files-1.2.1.9\"><span class=\"toc-item-num\">1.2.1.9&nbsp;&nbsp;</span>Exchange Metrics CSV-Files</a></span></li><li><span><a href=\"#Indicator-1-Metric-CSV-Files\" data-toc-modified-id=\"Indicator-1-Metric-CSV-Files-1.2.1.10\"><span class=\"toc-item-num\">1.2.1.10&nbsp;&nbsp;</span>Indicator-1 Metric CSV-Files</a></span></li><li><span><a href=\"#Indicator-2-Metrics-CSV-Files\" data-toc-modified-id=\"Indicator-2-Metrics-CSV-Files-1.2.1.11\"><span class=\"toc-item-num\">1.2.1.11&nbsp;&nbsp;</span>Indicator-2 Metrics CSV-Files</a></span></li><li><span><a href=\"#Indicator-3-Metrics-CSV-Files\" data-toc-modified-id=\"Indicator-3-Metrics-CSV-Files-1.2.1.12\"><span class=\"toc-item-num\">1.2.1.12&nbsp;&nbsp;</span>Indicator-3 Metrics CSV-Files</a></span></li><li><span><a href=\"#Addresses-Metric-CSV-Files\" data-toc-modified-id=\"Addresses-Metric-CSV-Files-1.2.1.13\"><span class=\"toc-item-num\">1.2.1.13&nbsp;&nbsp;</span>Addresses Metric CSV-Files</a></span></li><li><span><a href=\"#Futures-Metrics-CSV-Files---Excluded-as-Data-Available-Only-Since-Feb-2020\" data-toc-modified-id=\"Futures-Metrics-CSV-Files---Excluded-as-Data-Available-Only-Since-Feb-2020-1.2.1.14\"><span class=\"toc-item-num\">1.2.1.14&nbsp;&nbsp;</span>Futures Metrics CSV-Files - Excluded as Data Available Only Since Feb 2020</a></span><ul class=\"toc-item\"><li><span><a href=\"#List-of-Future-Features\" data-toc-modified-id=\"List-of-Future-Features-1.2.1.14.1\"><span class=\"toc-item-num\">1.2.1.14.1&nbsp;&nbsp;</span>List of Future Features</a></span></li><li><span><a href=\"#Counter-to-Differentiate-between-Data-Frame-Setup-and-Data-Frame-Extension\" data-toc-modified-id=\"Counter-to-Differentiate-between-Data-Frame-Setup-and-Data-Frame-Extension-1.2.1.14.2\"><span class=\"toc-item-num\">1.2.1.14.2&nbsp;&nbsp;</span>Counter to Differentiate between Data Frame Setup and Data Frame Extension</a></span></li><li><span><a href=\"#For-Loop-through-all-Features-in-the-List\" data-toc-modified-id=\"For-Loop-through-all-Features-in-the-List-1.2.1.14.3\"><span class=\"toc-item-num\">1.2.1.14.3&nbsp;&nbsp;</span>For Loop through all Features in the List</a></span></li><li><span><a href=\"#Store-Data-Frame-Copy-in-Market-Data-Frame\" data-toc-modified-id=\"Store-Data-Frame-Copy-in-Market-Data-Frame-1.2.1.14.4\"><span class=\"toc-item-num\">1.2.1.14.4&nbsp;&nbsp;</span>Store Data Frame Copy in Market Data Frame</a></span></li><li><span><a href=\"#Delete-Original-Data-Frame\" data-toc-modified-id=\"Delete-Original-Data-Frame-1.2.1.14.5\"><span class=\"toc-item-num\">1.2.1.14.5&nbsp;&nbsp;</span>Delete Original Data Frame</a></span></li><li><span><a href=\"#Change-Data-Type-of-Columns-to-Float\" data-toc-modified-id=\"Change-Data-Type-of-Columns-to-Float-1.2.1.14.6\"><span class=\"toc-item-num\">1.2.1.14.6&nbsp;&nbsp;</span>Change Data Type of Columns to Float</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Target-Variables\" data-toc-modified-id=\"Target-Variables-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Target Variables</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Volatility\" data-toc-modified-id=\"Volatility-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Volatility</a></span></li><li><span><a href=\"#Log-Price\" data-toc-modified-id=\"Log-Price-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Log Price</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Sets\" data-toc-modified-id=\"Data-Sets-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Set-I.---Blockchain-Metrics-Only\" data-toc-modified-id=\"Data-Set-I.---Blockchain-Metrics-Only-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Data Set I. - Blockchain Metrics Only</a></span></li></ul></li><li><span><a href=\"#Classification-Models---Data-Set-Variations\" data-toc-modified-id=\"Classification-Models---Data-Set-Variations-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Classification Models - Data Set Variations</a></span></li><li><span><a href=\"#Import-Data-Sets\" data-toc-modified-id=\"Import-Data-Sets-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Import Data Sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Set-I.\" data-toc-modified-id=\"Data-Set-I.-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Data Set I.</a></span></li><li><span><a href=\"#Prediction-Shift\" data-toc-modified-id=\"Prediction-Shift-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Prediction Shift</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Set-I.\" data-toc-modified-id=\"Data-Set-I.-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Data Set I.</a></span></li></ul></li></ul></li><li><span><a href=\"#Classification-Setup\" data-toc-modified-id=\"Classification-Setup-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classification Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Data-Set-Copies-for-Trading-Strategy\" data-toc-modified-id=\"Create-Data-Set-Copies-for-Trading-Strategy-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Create Data Set Copies for Trading Strategy</a></span></li><li><span><a href=\"#Categorization---Data-Set-I.\" data-toc-modified-id=\"Categorization---Data-Set-I.-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Categorization - Data Set I.</a></span></li></ul></li><li><span><a href=\"#Model-Preparation---Data-Set-I.\" data-toc-modified-id=\"Model-Preparation---Data-Set-I.-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Preparation - Data Set I.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-Test-Split---Data-Set-I.\" data-toc-modified-id=\"Train-Test-Split---Data-Set-I.-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Train Test Split - Data Set I.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-of-Class-Balance\" data-toc-modified-id=\"Check-of-Class-Balance-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Check of Class Balance</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-Selection-random-forest\" data-toc-modified-id=\"Feature-Selection-random-forest-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Feature Selection random forest</a></span></li><li><span><a href=\"#Visualize-pred-vs-reality\" data-toc-modified-id=\"Visualize-pred-vs-reality-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Visualize pred vs reality</a></span></li><li><span><a href=\"#Hyperparam-tuning\" data-toc-modified-id=\"Hyperparam-tuning-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Hyperparam tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Scaling---Data-Set-I.\" data-toc-modified-id=\"Feature-Scaling---Data-Set-I.-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Feature Scaling - Data Set I.</a></span></li></ul></li><li><span><a href=\"#Classification-Models---Data-Set-I.\" data-toc-modified-id=\"Classification-Models---Data-Set-I.-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Classification Models - Data Set I.</a></span><ul class=\"toc-item\"><li><span><a href=\"#CatBoostClassifier---Data-Set-I.\" data-toc-modified-id=\"CatBoostClassifier---Data-Set-I.-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>CatBoostClassifier - Data Set I.</a></span></li><li><span><a href=\"#XGBoost---Data-Set-I.\" data-toc-modified-id=\"XGBoost---Data-Set-I.-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>XGBoost - Data Set I.</a></span></li><li><span><a href=\"#Logistic-Regression---Data-Set-I.\" data-toc-modified-id=\"Logistic-Regression---Data-Set-I.-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Logistic Regression - Data Set I.</a></span></li><li><span><a href=\"#Random-Forest-Classifier---Data-Set-I.\" data-toc-modified-id=\"Random-Forest-Classifier---Data-Set-I.-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Random Forest Classifier - Data Set I.</a></span></li></ul></li><li><span><a href=\"#Trading-Strategy---Data-Set-I\" data-toc-modified-id=\"Trading-Strategy---Data-Set-I-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Trading Strategy - Data Set I</a></span><ul class=\"toc-item\"><li><span><a href=\"#Trading-Strategy-in-Combination-with-Logistic-Regression\" data-toc-modified-id=\"Trading-Strategy-in-Combination-with-Logistic-Regression-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Trading Strategy in Combination with Logistic Regression</a></span></li></ul></li><li><span><a href=\"#save-to-csv\" data-toc-modified-id=\"save-to-csv-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>save to csv</a></span></li><li><span><a href=\"#Deepdive-in-prediction\" data-toc-modified-id=\"Deepdive-in-prediction-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Deepdive in prediction</a></span></li><li><span><a href=\"#predict-on-tomorrow\" data-toc-modified-id=\"predict-on-tomorrow-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>predict on tomorrow</a></span></li><li><span><a href=\"#get-x-train-data-of-today-to-predict-tomorrow-y\" data-toc-modified-id=\"get-x-train-data-of-today-to-predict-tomorrow-y-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>get x train data of today to predict tomorrow y</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import via Application Programming Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.597024Z",
     "start_time": "2020-11-23T00:15:15.559633Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import of Modules and Packages\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "from pytrends import dailydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.602999Z",
     "start_time": "2020-11-23T00:15:16.598958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adjustment of Decimal Places\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.608957Z",
     "start_time": "2020-11-23T00:15:16.604971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Glassnode API Key\n",
    "API_Key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('\\\\github_repo\\\\JupyterLabDir\\\\Rest\\\\MA BTC\\\\Markus_Code_MA_final\\\\20201010\\\\Glassnode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.618461Z",
     "start_time": "2020-11-23T00:15:16.610974Z"
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.873781Z",
     "start_time": "2020-11-23T00:15:16.623418Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('{}JupyterLabDir\\\\Rest\\\\MA BTC\\\\Markus_Code_MA_final\\\\20201010\\\\Glassnode'.format(os.getcwd().split('JupyterLabDir')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockchain Data from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.880770Z",
     "start_time": "2020-11-23T00:15:15.570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Function for a Single Feature Variable from Glassnode\n",
    "def import_glassnode (url, feature_name):\n",
    "    data = requests.get(url).json()\n",
    "    df = pd.json_normalize(data)\n",
    "    \n",
    "    # Rename columns\n",
    "    df.columns = ['Date', feature_name]\n",
    "   \n",
    "    # Convert Object to Datetime Object\n",
    "    df.Date = pd.to_datetime(df.Date, unit='s')\n",
    "    df.Date = df.Date.map(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Set Date Column as Index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Drop Missing Values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Change Data Type to Float\n",
    "    df = df.astype(float)\n",
    "    \n",
    "    # Creation of CSV-File, Part of Feature Name as CSV File Name \n",
    "    df.to_csv(\"{}.csv\".format(feature_name.rsplit('/')[-1]), decimal=',')\n",
    "    \n",
    "    # Plot of DataFrame\n",
    "    #df.plot(figsize=(4.5, 1.75))\n",
    "    #plt.xticks(rotation=45)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.881727Z",
     "start_time": "2020-11-23T00:15:15.576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Function for Several Feature Variables from Glassnode\n",
    "def import_glassnode_many(feature_list):\n",
    "    # Counter to Differentiate between Data Frame Setup and Data Frame Join\n",
    "    counter=0\n",
    "    \n",
    "    # For Loop through all Features in the List\n",
    "    for feature in feature_list:\n",
    "        \n",
    "        # URL\n",
    "        url = 'https://api.glassnode.com/v1/metrics/{}?a=btc&f=json&api_key={}'.format(feature, API_Key)\n",
    "        \n",
    "        # Execute Import Function for a Single Feature Variable\n",
    "        df=import_glassnode(url, feature)\n",
    "        \n",
    "        # Display Data Frame Tail\n",
    "        print(df.tail())\n",
    "        \n",
    "        # Condition - Data Frame Set up or Data Frame Join\n",
    "        if counter > 0:\n",
    "            \n",
    "            # Data Frame Join\n",
    "            df_new = df_new.join(df, how='inner')   \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Data Frame Setup\n",
    "            df_new = df\n",
    "        \n",
    "        # Increase Counter\n",
    "        counter +=1\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.882719Z",
     "start_time": "2020-11-23T00:15:15.583Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Market Features\n",
    "market_feature_list = ['market/price_usd_close', 'market/price_drawdown_relative', 'market/price_realized_usd', 'market/mvrv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.884746Z",
     "start_time": "2020-11-23T00:15:15.589Z"
    }
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    # Data Frame of Market Features\n",
    "    df_market = import_glassnode_many(market_feature_list)\n",
    "except:\n",
    "    time.sleep(60)\n",
    "    df_market = import_glassnode_many(market_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mining Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.885715Z",
     "start_time": "2020-11-23T00:15:15.599Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Mining Features    #'mining/revenue_sum',\n",
    "mining_feature_list = ['mining/difficulty_latest', 'mining/hash_rate_mean', 'mining/marketcap_thermocap_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.886712Z",
     "start_time": "2020-11-23T00:15:15.604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Mining Features\n",
    "df_mining = import_glassnode_many(mining_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blocks Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.888754Z",
     "start_time": "2020-11-23T00:15:15.611Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Blockchain Features\n",
    "blockchain_feature_list = ['blockchain/block_count', 'blockchain/block_interval_mean', 'blockchain/block_size_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.889715Z",
     "start_time": "2020-11-23T00:15:15.617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Blockchain Features\n",
    "df_blockchainfeatures = import_glassnode_many(blockchain_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Metrics from Glassnode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.890728Z",
     "start_time": "2020-11-23T00:15:15.625Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Distribution Features\n",
    "#distribution_feature_list = ['distribution/balance_1pct_holders', 'distribution/gini', 'distribution/herfindahl'] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.891717Z",
     "start_time": "2020-11-23T00:15:15.631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Distribution Features\n",
    "#df_distribution = import_glassnode_many(distribution_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fee Metrics from Glassnode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.892716Z",
     "start_time": "2020-11-23T00:15:15.638Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Fee Features\n",
    "fees_feature_list = ['fees/volume_sum', 'fees/volume_mean', 'fees/fee_ratio_multiple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.894717Z",
     "start_time": "2020-11-23T00:15:15.644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Fee Features\n",
    "df_fees = import_glassnode_many(fees_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTXO Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.895713Z",
     "start_time": "2020-11-23T00:15:15.651Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of UTXO Features\n",
    "utxo_feature_list = ['blockchain/utxo_created_count', 'blockchain/utxo_spent_count', 'blockchain/utxo_created_value_sum', 'blockchain/utxo_spent_value_sum', 'blockchain/utxo_profit_relative', 'blockchain/utxo_profit_count', 'blockchain/utxo_loss_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.896746Z",
     "start_time": "2020-11-23T00:15:15.657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of UTXO Features\n",
    "df_utxo = import_glassnode_many(utxo_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.898726Z",
     "start_time": "2020-11-23T00:15:15.668Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Supply Features\n",
    "supply_feature_list = ['supply/current','supply/profit_relative', 'supply/profit_sum', 'supply/loss_sum', 'supply/active_more_1y_percent', 'supply/active_more_2y_percent', 'supply/active_more_3y_percent', 'supply/active_more_5y_percent', 'supply/active_24h', 'supply/active_1d_1w', 'supply/active_1w_1m', 'supply/active_1m_3m', 'supply/active_3m_6m', 'supply/active_6m_12m', 'supply/active_1y_2y', 'supply/active_2y_3y', 'supply/active_3y_5y', 'supply/active_5y_7y', 'supply/active_7y_10y', 'supply/active_more_10y', 'supply/issued', 'supply/inflation_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.899729Z",
     "start_time": "2020-11-23T00:15:15.674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Supply Features\n",
    "df_supply = import_glassnode_many(supply_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.901715Z",
     "start_time": "2020-11-23T00:15:15.680Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Transaction Features\n",
    "transaction_feature_list = ['transactions/count', 'transactions/size_sum','transactions/transfers_volume_sum', 'transactions/transfers_volume_adjusted_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.902717Z",
     "start_time": "2020-11-23T00:15:15.686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Transaction Features\n",
    "df_transaction = import_glassnode_many(transaction_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exchange Metrics from Glassnode - 1 Month Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.903716Z",
     "start_time": "2020-11-23T00:15:15.696Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Exchange Features\n",
    "#exchange_feature_list = ['transactions/transfers_volume_to_exchanges_sum', 'transactions/transfers_volume_from_exchanges_sum', 'transactions/transfers_to_exchanges_count', 'transactions/transfers_from_exchanges_count', 'distribution/balance_exchanges', 'transactions/transfers_volume_exchanges_net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.904716Z",
     "start_time": "2020-11-23T00:15:15.703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Exchange Features\n",
    "#df_exchange = import_glassnode_many(exchange_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator-I Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.905710Z",
     "start_time": "2020-11-23T00:15:15.710Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Indicator-1 Features\n",
    "indicator1_feature_list = ['indicators/sopr_adjusted', 'indicators/nvt', 'indicators/velocity', 'indicators/cdd', 'indicators/reserve_risk', 'indicators/average_dormancy', 'indicators/liveliness']                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.906716Z",
     "start_time": "2020-11-23T00:15:15.716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Indicator-1 Features\n",
    "df_indicator1 = import_glassnode_many(indicator1_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator-2 Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.908718Z",
     "start_time": "2020-11-23T00:15:15.723Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Indicator-2 Features\n",
    "indicator2_feature_list = ['indicators/asol', 'indicators/sol_1h', 'indicators/sol_1h_24h', 'indicators/sol_1d_1w', 'indicators/sol_1w_1m', 'indicators/sol_1m_3m', 'indicators/sol_3m_6m', 'indicators/sol_6m_12m', 'indicators/sol_1y_2y', 'indicators/sol_2y_3y', 'indicators/sol_3y_5y', 'indicators/sol_5y_7y', 'indicators/sol_7y_10y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.909754Z",
     "start_time": "2020-11-23T00:15:15.729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Indicator-2 Features\n",
    "df_indicator2 = import_glassnode_many(indicator2_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator-3 Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.910714Z",
     "start_time": "2020-11-23T00:15:15.739Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Indicator-3 Features\n",
    "indicator3_feature_list = ['indicators/net_unrealized_profit_loss', 'indicators/unrealized_profit', 'indicators/unrealized_loss', 'indicators/net_realized_profit_loss', 'indicators/realized_profit', 'indicators/realized_loss', 'indicators/nupl_more_155', 'indicators/nupl_less_155', 'indicators/puell_multiple', 'indicators/stock_to_flow_deflection', 'indicators/difficulty_ribbon_compression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.912728Z",
     "start_time": "2020-11-23T00:15:15.745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Indicator-3 Features\n",
    "df_indicator3 = import_glassnode_many(indicator3_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addresses Metrics from Glassnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.913747Z",
     "start_time": "2020-11-23T00:15:15.752Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Addresses Features\n",
    "addresses_feature_list = ['addresses/active_count', 'addresses/sending_count', 'addresses/receiving_count', 'addresses/new_non_zero_count', 'addresses/non_zero_count', 'addresses/min_point_zero_1_count','addresses/min_point_1_count', 'addresses/min_1_count', 'addresses/min_10_count', 'addresses/min_100_count', 'addresses/min_1k_count', 'addresses/min_10k_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.914716Z",
     "start_time": "2020-11-23T00:15:15.761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Frame of Addresses Features\n",
    "df_addresses = import_glassnode_many(addresses_feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Futures Metrics from Glassnode - Excluded as Data Available Only Since Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of Futures Features\n",
    "futures_feature_list = ['derivatives/futures_volume_daily_all_sum', 'derivatives/futures_volume_daily_perpetual_sum', 'derivatives/futures_open_interest_all_sum', 'derivatives/futures_open_interest_perpetual_sum', 'derivatives/futures_funding_rate_perpetual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Frame of Future features\n",
    "df_futures = import_glassnode_many(futures_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Import and Data Tidying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.915713Z",
     "start_time": "2020-11-23T00:15:15.776Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import of Modules and Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.917717Z",
     "start_time": "2020-11-23T00:15:15.783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adjustment of Decimal Places\n",
    "pd.options.display.float_format = '{:.6f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.918715Z",
     "start_time": "2020-11-23T00:15:15.789Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\markus.gebele\\\\github_repo\\\\JupyterLabDir\\\\Rest\\\\MA BTC\\\\Markus_Code_MA_final\\\\20201010')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Blockchain CSV-Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.918715Z",
     "start_time": "2020-11-23T00:15:15.798Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Market Features\n",
    "market_feature_list = ['price_usd_close', 'price_drawdown_relative', 'price_realized_usd', 'mvrv']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in market_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date', ) #decimal=\".\"\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Market Data Frame\n",
    "df_market = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_market.columns:\n",
    "    try:\n",
    "        df_market[\"{}\".format(y)] = df_market[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_market[\"{}\".format(y)] = df_market[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.920715Z",
     "start_time": "2020-11-23T00:15:15.804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Market Metric Values\n",
    "df_market.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mining Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.921717Z",
     "start_time": "2020-11-23T00:15:15.812Z"
    }
   },
   "outputs": [],
   "source": [
    "# # List of Mining Features\n",
    "# mining_feature_list = ['difficulty_latest'] #, 'hash_rate_mean', 'revenue_sum','marketcap_thermocap_ratio'\n",
    "\n",
    "# # For Loop through all Features in the List\n",
    "# for x in mining_feature_list:\n",
    "    \n",
    "#     # Import Feature CSV-Files\n",
    "#     df_mining_difficulty_latest = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "# # Change Data Type of Columns to Float\n",
    "# for y in df_mining_difficulty_latest.columns:\n",
    "#     try:\n",
    "#         df_mining_difficulty_latest[\"{}\".format(y)] = df_mining_difficulty_latest[\"{}\".format(y)].str.replace(',','.')\n",
    "#         df_mining_difficulty_latest[\"{}\".format(y)] = df_mining_difficulty_latest[\"{}\".format(y)].astype(float)\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "# df_mining_difficulty_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.923715Z",
     "start_time": "2020-11-23T00:15:15.819Z"
    }
   },
   "outputs": [],
   "source": [
    "# # List of Mining Features\n",
    "# mining_feature_list = ['hash_rate_mean'] # 'revenue_sum','marketcap_thermocap_ratio'\n",
    "\n",
    "# # For Loop through all Features in the List\n",
    "# for x in mining_feature_list:\n",
    "    \n",
    "#     # Import Feature CSV-Files\n",
    "#     df_mining_hash_rate_mean = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "# # Change Data Type of Columns to Float\n",
    "# for y in df_mining_hash_rate_mean.columns:\n",
    "#     try:\n",
    "#         df_mining_hash_rate_mean[\"{}\".format(y)] = df_mining_hash_rate_mean[\"{}\".format(y)].str.replace(',','.')\n",
    "#         df_mining_hash_rate_mean[\"{}\".format(y)] = df_mining_hash_rate_mean[\"{}\".format(y)].astype(float)\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "# df_mining_hash_rate_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.924717Z",
     "start_time": "2020-11-23T00:15:15.826Z"
    }
   },
   "outputs": [],
   "source": [
    "# # List of Mining Features\n",
    "# mining_feature_list = ['marketcap_thermocap_ratio'] # 'revenue_sum','marketcap_thermocap_ratio'\n",
    "\n",
    "# # For Loop through all Features in the List\n",
    "# for x in mining_feature_list:\n",
    "    \n",
    "#     # Import Feature CSV-Files\n",
    "#     df_mining_marketcap_thermocap_ratio = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "# # Change Data Type of Columns to Float\n",
    "# for y in df_mining_marketcap_thermocap_ratio.columns:\n",
    "#     try:\n",
    "#         df_mining_marketcap_thermocap_ratio[\"{}\".format(y)] = df_mining_marketcap_thermocap_ratio[\"{}\".format(y)].str.replace(',','.')\n",
    "#         df_mining_marketcap_thermocap_ratio[\"{}\".format(y)] = df_mining_marketcap_thermocap_ratio[\"{}\".format(y)].astype(float)\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "# df_mining_revenue_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.925716Z",
     "start_time": "2020-11-23T00:15:15.833Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Mining Features\n",
    "mining_feature_list = ['difficulty_latest', 'hash_rate_mean'] #, 'revenue_sum','marketcap_thermocap_ratio'\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in mining_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Market Data Frame    \n",
    "df_mining = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_mining.columns:\n",
    "    try:\n",
    "        df_mining[\"{}\".format(y)] = df_mining[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_mining[\"{}\".format(y)] = df_mining[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.926716Z",
     "start_time": "2020-11-23T00:15:15.842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Mining Metric Values\n",
    "df_mining.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blocks Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.927714Z",
     "start_time": "2020-11-23T00:15:15.849Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Blockchain Features\n",
    "block_feature_list = ['block_count', 'block_interval_mean', 'block_size_sum']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in block_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Market Data Frame\n",
    "df_blockfeatures = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_blockfeatures.columns:\n",
    "    try:\n",
    "        df_blockfeatures[\"{}\".format(y)] = df_blockfeatures[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_blockfeatures[\"{}\".format(y)] = df_blockfeatures[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.929715Z",
     "start_time": "2020-11-23T00:15:15.856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show Latest Feature Values\n",
    "df_blockfeatures.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.930715Z",
     "start_time": "2020-11-23T00:15:15.863Z"
    }
   },
   "outputs": [],
   "source": [
    "# # List of Distribution Features\n",
    "# distribution_feature_list = ['balance_1pct_holders', 'gini', 'herfindahl']\n",
    "\n",
    "# # Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "# counter=0\n",
    "\n",
    "# # For Loop through all Features in the List\n",
    "# for x in distribution_feature_list:\n",
    "    \n",
    "#     # Import Feature CSV-Files\n",
    "#     df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "#     # Condition - Data Frame Set up or Data Frame Extension\n",
    "#     if counter > 0:\n",
    "        \n",
    "#         # Data Frame Extension - Inner Join\n",
    "#         df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "#     else:\n",
    "#         # Data Frame Setup\n",
    "#         df_new = df\n",
    "    \n",
    "#     # Increase Counter    \n",
    "#     counter +=1\n",
    "\n",
    "# # Store Data Frame Copy in Distribution Data Frame    \n",
    "# df_distribution = df_new.copy()\n",
    "\n",
    "# # Delete Original Data Frame\n",
    "# del df_new\n",
    "\n",
    "# # Change Data Type of Columns to Float\n",
    "# for y in df_distribution.columns:\n",
    "#     try:\n",
    "#         df_distribution[\"{}\".format(y)] = df_distribution[\"{}\".format(y)].str.replace(',','.')\n",
    "#         df_distribution[\"{}\".format(y)] = df_distribution[\"{}\".format(y)].astype(float)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.931719Z",
     "start_time": "2020-11-23T00:15:15.869Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Latest Distribution Metric Values\n",
    "# df_distribution.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fee Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.932714Z",
     "start_time": "2020-11-23T00:15:15.877Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Fee Features\n",
    "fees_feature_list = ['volume_sum', 'volume_mean', 'fee_ratio_multiple']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in fees_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date') #decimal=\".\"\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Fees Data Frame    \n",
    "df_fees = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_fees.columns:\n",
    "    try:\n",
    "        df_fees[\"{}\".format(y)] = df_fees[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_fees[\"{}\".format(y)] = df_fees[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.934717Z",
     "start_time": "2020-11-23T00:15:15.883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Fee Metric Values\n",
    "df_fees.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTXO Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.935713Z",
     "start_time": "2020-11-23T00:15:15.890Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of UTXO Features\n",
    "utxo_feature_list = ['utxo_created_count', 'utxo_spent_count','utxo_created_value_sum', 'utxo_spent_value_sum', 'utxo_profit_relative', 'utxo_profit_count', 'utxo_loss_count']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in utxo_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date', ) #decimal=\".\"\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "\n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in UTXO Data Frame\n",
    "df_utxo = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_utxo.columns:\n",
    "    try:\n",
    "        df_utxo[\"{}\".format(y)] = df_utxo[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_utxo[\"{}\".format(y)] = df_utxo[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.936717Z",
     "start_time": "2020-11-23T00:15:15.896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest UTXO Metric Values\n",
    "df_utxo.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.937713Z",
     "start_time": "2020-11-23T00:15:15.904Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Supply Features\n",
    "supply_feature_list = ['current','profit_relative', 'profit_sum', 'loss_sum', 'active_more_1y_percent', 'active_more_2y_percent', 'active_more_3y_percent', 'active_more_5y_percent', 'active_24h', 'active_1d_1w', 'active_1w_1m', 'active_1m_3m', 'active_3m_6m', 'active_6m_12m', 'active_1y_2y', 'active_2y_3y', 'active_3y_5y', 'active_5y_7y', 'active_7y_10y', 'active_more_10y', 'issued', 'inflation_rate']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in supply_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date', ) #decimal=\".\"\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Supply Data Frame    \n",
    "df_supply = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_supply.columns:\n",
    "    try:\n",
    "        df_supply[\"{}\".format(y)] = df_supply[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_supply[\"{}\".format(y)] = df_supply[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.939721Z",
     "start_time": "2020-11-23T00:15:15.911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Supply Metric Values\n",
    "df_supply.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.940718Z",
     "start_time": "2020-11-23T00:15:15.917Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Transaction features\n",
    "transaction_feature_list = ['count', 'size_sum','transfers_volume_sum', 'transfers_volume_adjusted_sum']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in transaction_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date', ) #decimal=\".\"\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Transaction Data Frame\n",
    "df_transaction = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_transaction.columns:\n",
    "    try:\n",
    "        df_transaction[\"{}\".format(y)] = df_transaction[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_transaction[\"{}\".format(y)] = df_transaction[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.941715Z",
     "start_time": "2020-11-23T00:15:15.923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Transaction Metric Values\n",
    "df_transaction.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exchange Metrics CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.942719Z",
     "start_time": "2020-11-23T00:15:15.930Z"
    }
   },
   "outputs": [],
   "source": [
    "# # List of Exchange Features\n",
    "# exchange_feature_list = ['transfers_volume_to_exchanges_sum', 'transfers_volume_from_exchanges_sum', 'transfers_to_exchanges_count', \n",
    "#                          'transfers_from_exchanges_count', 'balance_exchanges', 'transfers_volume_exchanges_net']\n",
    "\n",
    "# # Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "# counter=0\n",
    "\n",
    "# # For Loop through all Features in the List\n",
    "# for x in exchange_feature_list:\n",
    "    \n",
    "#     # Import Feature CSV-Files\n",
    "#     df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date', ) #decimal=\".\"\n",
    "    \n",
    "#     # Condition - Data Frame Set up or Data Frame Extension\n",
    "#     if counter > 0:\n",
    "        \n",
    "#         # Data Frame Extension - Inner Join\n",
    "#         df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "#     else:\n",
    "        \n",
    "#         # Data Frame Setup\n",
    "#         df_new = df\n",
    "    \n",
    "#     # Increase Counter\n",
    "#     counter +=1\n",
    "\n",
    "# # Store Data Frame Copy in Exchange Data Frame    \n",
    "# df_exchange = df_new.copy()\n",
    "\n",
    "# # Delete Original Data Frame\n",
    "# del df_new\n",
    "\n",
    "# # Change Data Type of Columns to Float\n",
    "# for y in df_exchange.columns:\n",
    "#     try:\n",
    "#         df_exchange[\"{}\".format(y)] = df_exchange[\"{}\".format(y)].str.replace(',','.')\n",
    "#         df_exchange[\"{}\".format(y)] = df_exchange[\"{}\".format(y)].astype(float)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.943740Z",
     "start_time": "2020-11-23T00:15:15.935Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Latest Exchange Metric Values\n",
    "# df_exchange.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator-1 Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.945718Z",
     "start_time": "2020-11-23T00:15:15.942Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Indicator-1 features\n",
    "indicator1_feature_list = ['sopr_adjusted', 'nvt', 'velocity', 'cdd','reserve_risk', 'average_dormancy', 'liveliness'] \n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in indicator1_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date', ) #decimal=\".\"\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Indicator-1 Data Frame    \n",
    "df_indicator1 = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_indicator1.columns:\n",
    "    try:\n",
    "        df_indicator1[\"{}\".format(y)] = df_indicator1[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_indicator1[\"{}\".format(y)] = df_indicator1[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.946718Z",
     "start_time": "2020-11-23T00:15:15.949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Indicator-1 Metric Values\n",
    "df_indicator1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator-2 Metrics CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.947715Z",
     "start_time": "2020-11-23T00:15:15.955Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Indicator-2 Features\n",
    "indicator2_feature_list = ['asol', 'sol_1h', 'sol_1h_24h', 'sol_1d_1w','sol_1w_1m', 'sol_1m_3m', 'sol_3m_6m', 'sol_6m_12m','sol_1y_2y', 'sol_2y_3y', 'sol_3y_5y', 'sol_5y_7y', 'sol_7y_10y']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in indicator2_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Indicator-2 Data Frame\n",
    "df_indicator2 = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_indicator2.columns:\n",
    "    try:\n",
    "        df_indicator2[\"{}\".format(y)] = df_indicator2[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_indicator2[\"{}\".format(y)] = df_indicator2[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.949744Z",
     "start_time": "2020-11-23T00:15:15.962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Indicator-2 Metric Values\n",
    "df_indicator2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator-3 Metrics CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.950716Z",
     "start_time": "2020-11-23T00:15:15.969Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Indicator-3 Features\n",
    "indicator3_feature_list = ['net_unrealized_profit_loss', 'unrealized_profit', 'unrealized_loss', 'net_realized_profit_loss', 'realized_profit', 'realized_loss', 'nupl_more_155', 'nupl_less_155', 'puell_multiple', 'stock_to_flow_deflection', 'difficulty_ribbon_compression']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in indicator3_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Indicator-3 Data Frame\n",
    "df_indicator3 = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_indicator3.columns:\n",
    "    try:\n",
    "        df_indicator3[\"{}\".format(y)] = df_indicator3[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_indicator3[\"{}\".format(y)] = df_indicator3[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.951716Z",
     "start_time": "2020-11-23T00:15:15.975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Indicator-3 Metric Values\n",
    "df_indicator3.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addresses Metric CSV-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.953718Z",
     "start_time": "2020-11-23T00:15:15.982Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Addresses Features\n",
    "addresses_feature_list = ['active_count', 'sending_count', 'receiving_count', 'new_non_zero_count', 'non_zero_count', 'min_point_zero_1_count', 'min_point_1_count', 'min_1_count', 'min_10_count', 'min_100_count', 'min_1k_count', 'min_10k_count']\n",
    "\n",
    "# Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "# For Loop through all Features in the List\n",
    "for x in addresses_feature_list:\n",
    "    \n",
    "    # Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "    # Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        # Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    # Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "# Store Data Frame Copy in Addresses Data Frame\n",
    "df_addresses = df_new.copy()\n",
    "\n",
    "# Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "# Change Data Type of Columns to Float\n",
    "for y in df_addresses.columns:\n",
    "    try:\n",
    "        df_addresses[\"{}\".format(y)] = df_addresses[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_addresses[\"{}\".format(y)] = df_addresses[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.954714Z",
     "start_time": "2020-11-23T00:15:15.988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Latest Addresses Metric Values\n",
    "df_addresses.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Futures Metrics CSV-Files - Excluded as Data Available Only Since Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of Future Features\n",
    "futures_feature_list = ['futures_volume_daily_all_sum', 'futures_volume_daily_perpetual_sum', 'futures_open_interest_all_sum', 'futures_open_interest_perpetual_sum', 'futures_funding_rate_perpetual']\n",
    "\n",
    "##### Counter to Differentiate between Data Frame Setup and Data Frame Extension\n",
    "counter=0\n",
    "\n",
    "##### For Loop through all Features in the List\n",
    "for x in futures_feature_list:\n",
    "    \n",
    "    ##### Import Feature CSV-Files\n",
    "    df = pd.read_csv(\"Glassnode/{}.csv\".format(x), index_col='Date')\n",
    "    \n",
    "    ##### Condition - Data Frame Set up or Data Frame Extension\n",
    "    if counter > 0:\n",
    "        \n",
    "        ##### Data Frame Extension - Inner Join\n",
    "        df_new = df_new.join(df, how='inner')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ##### Data Frame Setup\n",
    "        df_new = df\n",
    "    \n",
    "    ##### Increase Counter\n",
    "    counter +=1\n",
    "\n",
    "##### Store Data Frame Copy in Market Data Frame\n",
    "df_futures = df_new.copy()\n",
    "\n",
    "##### Delete Original Data Frame\n",
    "del df_new\n",
    "\n",
    "##### Change Data Type of Columns to Float\n",
    "for y in df_futures.columns:\n",
    "    try:\n",
    "        df_futures[\"{}\".format(y)] = df_futures[\"{}\".format(y)].str.replace(',','.')\n",
    "        df_futures[\"{}\".format(y)] = df_futures[\"{}\".format(y)].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.956734Z",
     "start_time": "2020-11-23T00:15:15.998Z"
    }
   },
   "outputs": [],
   "source": [
    "df_return = pd.DataFrame(df_market['market/price_usd_close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.957715Z",
     "start_time": "2020-11-23T00:15:16.004Z"
    }
   },
   "outputs": [],
   "source": [
    "df_return.columns = ['Closed Price USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.958715Z",
     "start_time": "2020-11-23T00:15:16.010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Column with BTC Daily Returns in USD\n",
    "df_return['Daily Return in USD'] = df_return['Closed Price USD'].diff()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add Column with BTC Daily Returns in Percent\n",
    "df_return['Daily Returns in Percent'] = (df_return['Daily Returns USD']) / (df_return['Closed Price USD'].shift(1))*100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add Column with BTC Daily Log Returns in Percent\n",
    "df_return['Daily Log Returns in Percent'] = pd.DataFrame(np.log(df_return['Closed Price USD']/df_return['Closed Price USD'].shift(1))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.960715Z",
     "start_time": "2020-11-23T00:15:16.169Z"
    }
   },
   "outputs": [],
   "source": [
    "df_return.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.961736Z",
     "start_time": "2020-11-23T00:15:16.177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting BTC Close Price in USD\n",
    "df_volatility = pd.DataFrame(df_market['market/price_usd_close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.962716Z",
     "start_time": "2020-11-23T00:15:16.184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename Price Column\n",
    "df_volatility.columns = ['Closed Price USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.964712Z",
     "start_time": "2020-11-23T00:15:16.191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Column with BTC Daily Returns in USD\n",
    "df_volatility['Daily Return in USD'] = df_volatility['Closed Price USD'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.965713Z",
     "start_time": "2020-11-23T00:15:16.198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Column with BTC Daily Returns in Percent\n",
    "df_volatility['Daily Return in Percent'] = (df_volatility['Daily Return in USD']) / (df_volatility['Closed Price USD'].shift(1))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.966713Z",
     "start_time": "2020-11-23T00:15:16.205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Column with BTC Daily Log Returns in Percent\n",
    "df_volatility['Daily Log Return in Percent'] = pd.DataFrame(np.log(df_volatility['Closed Price USD']/df_volatility['Closed Price USD'].shift(1))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.967713Z",
     "start_time": "2020-11-23T00:15:16.211Z"
    }
   },
   "outputs": [],
   "source": [
    "df_volatility.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.968713Z",
     "start_time": "2020-11-23T00:15:16.217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculation of Rolling Volatility of BTC Daily Log Returns for 10, 20 and 30 days period\n",
    "df_volatility['Volatility Daily Log Return in Percent 3D'] = pd.DataFrame(df_volatility['Daily Log Return in Percent'].rolling(window=3, min_periods=1).std())\n",
    "df_volatility['Volatility Daily Log Return in Percent 5D'] = pd.DataFrame(df_volatility['Daily Log Return in Percent'].rolling(window=5, min_periods=1).std())\n",
    "df_volatility['Volatility Daily Log Return in Percent 10D'] = pd.DataFrame(df_volatility['Daily Log Return in Percent'].rolling(window=10, min_periods=1).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.969716Z",
     "start_time": "2020-11-23T00:15:16.224Z"
    }
   },
   "outputs": [],
   "source": [
    "df_volatility.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.971735Z",
     "start_time": "2020-11-23T00:15:16.230Z"
    }
   },
   "outputs": [],
   "source": [
    "df_volatility_reduced = df_volatility['Volatility Daily Log Return in Percent 5D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.972717Z",
     "start_time": "2020-11-23T00:15:16.238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting BTC Close Price in USD\n",
    "df_logprice = pd.DataFrame(np.log(df_market['market/price_usd_close']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.973716Z",
     "start_time": "2020-11-23T00:15:16.244Z"
    }
   },
   "outputs": [],
   "source": [
    "df_logprice.columns = ['Log Price in USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.974715Z",
     "start_time": "2020-11-23T00:15:16.252Z"
    }
   },
   "outputs": [],
   "source": [
    "df_logprice.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set I. - Blockchain Metrics Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.975715Z",
     "start_time": "2020-11-23T00:15:16.262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging all Blockchain DataFrame Metrics\n",
    "df_blockchain = pd.concat([df_return, df_logprice, df_volatility_reduced, df_market, df_mining, df_blockfeatures, df_fees, df_utxo, df_supply, \n",
    "                           df_transaction, df_indicator1, df_indicator2, df_indicator3, df_addresses], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.976713Z",
     "start_time": "2020-11-23T00:15:16.268Z"
    }
   },
   "outputs": [],
   "source": [
    "df_blockchain = df_blockchain.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.978713Z",
     "start_time": "2020-11-23T00:15:16.274Z"
    }
   },
   "outputs": [],
   "source": [
    "df_blockchain = df_blockchain.drop(['Closed Price USD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.979713Z",
     "start_time": "2020-11-23T00:15:16.280Z"
    }
   },
   "outputs": [],
   "source": [
    "df_blockchain.index = df_blockchain.index.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.981713Z",
     "start_time": "2020-11-23T00:15:16.285Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_blockchain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.982716Z",
     "start_time": "2020-11-23T00:15:16.291Z"
    }
   },
   "outputs": [],
   "source": [
    "df_blockchain.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.983715Z",
     "start_time": "2020-11-23T00:15:16.298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summary of each variable\n",
    "df_blockchain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T00:15:16.984716Z",
     "start_time": "2020-11-23T00:15:16.304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating CSV-File of Blockchain Data\n",
    "df_blockchain.to_csv(\"df_dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models - Data Set Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:37:09.816542Z",
     "start_time": "2020-12-27T11:33:00.411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import of Modules and Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pycaret.classification import *\n",
    "\n",
    "# Deep Neural Networks\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# PyCaret\n",
    "import pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-27T11:37:09.818537Z",
     "start_time": "2020-12-27T11:33:01.244Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adjustment of Decimal Places\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:50.776Z"
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:50.790Z"
    }
   },
   "outputs": [],
   "source": [
    "# CSV Import of Data Set I. - Blockchain Data\n",
    "df_dataset1 = pd.read_csv('df_dataset1.csv', parse_dates = True, index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:50.805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Daily Return in USD in row 2011-07-16 means the change in \"btc dollar price\" from 2011-07-15 23:59 to 2011-07-16 23:59 \n",
    "# in row 2011-07-17 means the change in \"btc dollar price\" from 2011-07-16 23:59 to 2011-07-17 23:59 \n",
    "\n",
    "df_dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:50.818Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Set Info\n",
    "df_dataset1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:50.830Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of Data Types\n",
    "l_dtypes = df_dataset1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Counter\n",
    "counter = 0\n",
    "\n",
    "# Cross-Check of Data Types\n",
    "for i in l_dtypes:\n",
    "    if (i == float):\n",
    "        counter = counter+1\n",
    "    else:\n",
    "        counter = counter\n",
    "\n",
    "# Number of Float Data Types\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Types\n",
    "df_dataset1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "df_dataset1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Tail\n",
    "df_dataset1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.103Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dataset1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.115Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset1[['Daily Return in USD', 'Log Price in USD',\n",
    "       'Volatility Daily Log Return in Percent 5D', 'market/price_usd_close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shift of Target Column by 1 Day\n",
    "df_dataset1['Daily Return in USD_ohneshift'] = df_dataset1['Daily Return in USD'] \n",
    "df_dataset1[\"percentage_daily_return_bef_shift\"] = df_dataset1[\"Daily Return in USD_ohneshift\"]/df_dataset1[\"market/price_usd_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.138Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset1 = df_dataset1.sort_index(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.148Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset1['Daily Return in USD'] = df_dataset1['Daily Return in USD'].shift(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.161Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset1[['Daily Return in USD', 'addresses/active_count', 'Log Price in USD',\n",
    "       'Volatility Daily Log Return in Percent 5D', 'market/price_usd_close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.172Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset1['market/price_usd_close_cummax']=df_dataset1['market/price_usd_close'].cummax()\n",
    "df_dataset1['price_usd_close_percent_of_maxtilnow'] = df_dataset1['market/price_usd_close']/df_dataset1['market/price_usd_close_cummax']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Shift\n",
    "df_dataset1[[ 'Log Price in USD',\n",
    "       'Volatility Daily Log Return in Percent 5D', 'market/price_usd_close',\"percentage_daily_return_bef_shift\"]].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Empty Cells\n",
    "df_dataset1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.214Z"
    }
   },
   "outputs": [],
   "source": [
    "# daily return: von gestern auf heute die preisvernderung deswegen shift spter\n",
    "df_dataset1.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete Empty Row\n",
    "df_dataset1 = df_dataset1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-Check Empty Cells\n",
    "df_dataset1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Set Copies for Trading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Set Copy for Trading Extension\n",
    "df_dataset1_copy = df_dataset1.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categorization\n",
    "# Class 1 for all Returns >= 0 USD\n",
    "df_dataset1.loc[df_dataset1['Daily Return in USD'] >= 0, 'Class'] = 1\n",
    "# Class 0 for all Returns < 0 USD\n",
    "df_dataset1.loc[df_dataset1['Daily Return in USD'] < 0, 'Class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Set Head\n",
    "df_dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.278Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in range(len(df_dataset1.columns)):\n",
    "    print(x, df_dataset1.columns[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete Original Target Column\n",
    "df_dataset1 = df_dataset1.drop(['Daily Return in USD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Class Balance\n",
    "df_dataset1['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Set Head\n",
    "df_dataset1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation - Data Set I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup of Predictors (Indpendent Variables) by Excluding Target Variable\n",
    "predictors = df_dataset1.drop(['Class'], axis=1).values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup of Categorized Target Variable\n",
    "target = df_dataset1['Class'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predictor Dimensions\n",
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.351Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in range(len(predictors[0])):\n",
    "    print(x, predictors[0][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target Dimensions\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train-Test-Split Function to Create Training and Test Data Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state = 41, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions of Predictors for Training\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions of Target for Training\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of Predictors\n",
    "n_nodes = predictors.shape[1]\n",
    "n_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check of Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Training Target of Class 0\n",
    "y_train[y_train==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Training Target of Class 1\n",
    "y_train[y_train==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Test Target of Class 0\n",
    "y_test[y_test==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check Test Target of Class 1\n",
    "y_test[y_test==1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.437Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# rank_delete_list = []\n",
    "# df_dataset1_columns_keep = []\n",
    "\n",
    "\n",
    "# # define dataset\n",
    "# #X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# # define RFE\n",
    "# sel = SelectFromModel(RandomForestClassifier(max_depth=4, min_samples_leaf=4, min_samples_split=3,\n",
    "#                        n_estimators=625))\n",
    "# # fit RFE\n",
    "# sel.fit(X_train, y_train)\n",
    "# # summarize all features\n",
    "# sel.get_support()\n",
    "\n",
    "# # summarize all features\n",
    "# for i in range(X_train.shape[1]):\n",
    "#     if sel.get_support()[i] == False:\n",
    "#         rank_delete_list.append(i)\n",
    "#     else:\n",
    "#         print('Column: %d, Selected %s, col %s' % (i, sel.get_support()[i], df_dataset1.columns[i] ))\n",
    "#         df_dataset1_columns_keep.append(df_dataset1.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.445Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_dataset1_columns_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.453Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_dataset1=df_dataset1[df_dataset1_columns_keep]\n",
    "# df_dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.464Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(rank_delete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.475Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(df_dataset1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.486Z"
    }
   },
   "outputs": [],
   "source": [
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.496Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictors = np.delete(predictors, rank_delete_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.507Z"
    }
   },
   "outputs": [],
   "source": [
    "predictors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize pred vs reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data into Training and Testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    predictors, target, test_size=0.3, stratify=target)\n",
    "\n",
    "# Amount of Columns\n",
    "n_nodes = predictors.shape[1]\n",
    "\n",
    "# Instantiation of MinMaxScaler\n",
    "norm_scaler = MinMaxScaler()\n",
    "\n",
    "# Training and Application of Feature Scaler on Training Data of Predictors\n",
    "X_train_scaled = pd.DataFrame(norm_scaler.fit_transform(X_train))\n",
    "\n",
    "# Application of Feature Scaler on Test Data of Predictors\n",
    "X_test_scaled = norm_scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.532Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# LR = LogisticRegression()\n",
    "# LRparam_grid = {\n",
    "#     'C': [1], #1\n",
    "#      'penalty': ['l1'], #l2\n",
    "#      'max_iter': [1200, 12000],\n",
    "#      'solver': ['liblinear',] #,  'sag', 'saga' , 'lbfgs'  #'newton-cg'\n",
    "# }\n",
    "# LR_search = GridSearchCV(LR, param_grid=LRparam_grid, refit = True, verbose = 3, cv=5)\n",
    "\n",
    "# # fitting the model for grid search \n",
    "# LR_search.fit(X_train , y_train)\n",
    "# LR_search.best_params_\n",
    "# # summarize\n",
    "# print('Mean Accuracy: %.3f' % LR_search.best_score_)\n",
    "# print('Config: %s' % LR_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.541Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# LR = LogisticRegression()\n",
    "# LRparam_grid = {\n",
    "#     'C': [1], #1\n",
    "#      'penalty': ['l1'], #l2\n",
    "#      'max_iter': [1100],\n",
    "#      'solver': ['liblinear',] #,  'sag', 'saga' , 'lbfgs'  #'newton-cg'\n",
    "# }\n",
    "# LR_search = GridSearchCV(LR, param_grid=LRparam_grid, refit = True, verbose = 3, cv=5)\n",
    "\n",
    "# # fitting the model for grid search \n",
    "# LR_search.fit(X_train , y_train)\n",
    "# LR_search.best_params_\n",
    "# # summarize\n",
    "# print('Mean Accuracy: %.3f' % LR_search.best_score_)\n",
    "# print('Config: %s' % LR_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.560Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Classifier Logistic Regression\n",
    "lreg = LogisticRegression(max_iter=1200)\n",
    "\n",
    "# Train Classifier\n",
    "lreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction of Classes\n",
    "y_pred = lreg.predict(X_test_scaled)\n",
    "\n",
    "# Display Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Prediction of Probabilities\n",
    "y_pred_prob = lreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Create Data Frame of Actual Values - y_test\n",
    "df_y_test = pd.DataFrame(y_test)\n",
    "\n",
    "# Add Predicted Probabilities to Data Frame\n",
    "df_y_test[\"y_pred_prob\"] = y_pred_prob\n",
    "\n",
    "# Add Predicted Classes (0 or 1) to Data Frame to Cross-Check Predicted Probabilities\n",
    "df_y_test[\"y_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.575Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test=df_y_test.sort_index()\n",
    "df_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.583Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test[df_y_test['Class']!=df_y_test['y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.591Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test[df_y_test['Class']==df_y_test['y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.606Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_y_test.index, y=df_y_test['Class'],\n",
    "                    mode='lines+markers',\n",
    "                    name='Class - reality'))\n",
    "fig.add_trace(go.Scatter(x=df_y_test.index, y=df_y_test['y_pred_prob'],\n",
    "                    mode='lines+markers',\n",
    "                    name='y_pred'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Actual Daily Returns in USD from the Data Set Copy\n",
    "df_dataset1_copy_dailyreturns = df_dataset1_copy[['Daily Return in USD']]\n",
    "\n",
    "# # Merge Predictions and Daily Returns Data Frame on the Column \"Date\"\n",
    "# df_y_test_withdailyreturns = pd.merge(\n",
    "#     df_y_test, df_dataset1_copy_dailyreturns, on='Date')\n",
    "\n",
    "# df_dataset1_copy_percentage = df_dataset1_copy[['percentage_daily_return']]\n",
    "# df_y_test_withdailyreturns = pd.merge(\n",
    "#     df_y_test_withdailyreturns, df_dataset1_copy_percentage, on='Date')\n",
    "\n",
    "# # Add Bitcoin Price in USD\n",
    "# df_y_test_withdailyreturns = pd.merge(\n",
    "#     df_y_test_withdailyreturns, df_dataset1['market/price_usd_close'], on='Date')\n",
    "\n",
    "# df_y_test_withdailyreturns['y_pred_prob_bereinigt'] = np.where(\n",
    "#     df_y_test_withdailyreturns['y_pred_prob'] < 0.5, 1-df_y_test_withdailyreturns['y_pred_prob'], df_y_test_withdailyreturns['y_pred_prob'])\n",
    "\n",
    "# df_y_test_withdailyreturns[\"Einsatz\"] = einsatz * \\\n",
    "#     df_y_test_withdailyreturns[\"y_pred_prob_bereinigt\"]  # **2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiation of MinMaxScaler and StandardSacler\n",
    "norm_scaler = MinMaxScaler()\n",
    "#std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaler Training and Transformation of Training Set\n",
    "X_train_scaled = pd.DataFrame(norm_scaler.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaler Transformation of Test Set\n",
    "X_test_scaled = norm_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaled Test Features\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaler Transformation of Entire Training Set - Input for Cross-Validation\n",
    "predictors_scaled = norm_scaler.transform(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function To Show Accuracy Mean and Accuracy Standard Deviation\n",
    "def show_scores(scores):\n",
    "    # Scores Series\n",
    "    print('Scores:', scores)\n",
    "    # Score Mean\n",
    "    print('Mean:', np.mean(scores))\n",
    "    # Score Standard Deviation\n",
    "    print('Standard Deviation:', np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoostClassifier - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.686Z"
    }
   },
   "outputs": [],
   "source": [
    "# catboost for classification\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# evaluate the model\n",
    "model = CatBoostClassifier(verbose=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Classes\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Probabilities\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Predicted Probabilities\n",
    "y_pred_prob[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Actual Classes\n",
    "np.array(y_test[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Accuracy\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Accuracy\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.755Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output of Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output of Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.777Z"
    }
   },
   "outputs": [],
   "source": [
    "# 10-fold Cross Validation including Stratification\n",
    "skf = StratifiedKFold(shuffle=True, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print AUC Score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for AUC scores\n",
    "cv_auc = cross_val_score(lreg, predictors_scaled, target, cv=skf, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of AUC scores\n",
    "print(\"AUC scores computed using 10-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for Accuracy Scores\n",
    "cv_acc = cross_val_score(lreg, predictors_scaled, target, cv=skf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of Accuracy Scores\n",
    "print(\"Accuracy scores computed using 10-fold cross-validation: {}\".format(cv_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.821Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Accuracy Results - Mean and Standard Deviation\n",
    "show_scores(cv_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.833Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Instantiate Classifier - LogisticRegression\n",
    "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
    "              min_child_weight=1, monotone_constraints='()',\n",
    "              n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
    "              objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Classes\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Probabilities\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Predicted Probabilities\n",
    "y_pred_prob[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Actual Classes\n",
    "np.array(y_test[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Accuracy\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.886Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Accuracy\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.893Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output of Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output of Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.916Z"
    }
   },
   "outputs": [],
   "source": [
    "# 10-fold Cross Validation including Stratification\n",
    "skf = StratifiedKFold(shuffle=True, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print AUC Score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for AUC scores\n",
    "cv_auc = cross_val_score(lreg, predictors_scaled, target, cv=skf, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of AUC scores\n",
    "print(\"AUC scores computed using 10-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for Accuracy Scores\n",
    "cv_acc = cross_val_score(lreg, predictors_scaled, target, cv=skf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of Accuracy Scores\n",
    "print(\"Accuracy scores computed using 10-fold cross-validation: {}\".format(cv_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.957Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Accuracy Results - Mean and Standard Deviation\n",
    "show_scores(cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Classifier - LogisticRegression\n",
    "lreg = LogisticRegression(max_iter=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "lreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Classes\n",
    "y_pred = lreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Probabilities\n",
    "y_pred_prob = lreg.predict_proba(X_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:51.993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Predicted Probabilities\n",
    "y_pred_prob[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Actual Classes\n",
    "np.array(y_test[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Accuracy\n",
    "lreg.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Accuracy\n",
    "lreg.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.018Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output of Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.025Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output of Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.037Z"
    }
   },
   "outputs": [],
   "source": [
    "# 10-fold Cross Validation including Stratification\n",
    "skf = StratifiedKFold(shuffle=True, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print AUC Score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for AUC scores\n",
    "cv_auc = cross_val_score(lreg, predictors_scaled, target, cv=skf, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of AUC scores\n",
    "print(\"AUC scores computed using 10-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.062Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for Accuracy Scores\n",
    "cv_acc = cross_val_score(lreg, predictors_scaled, target, cv=skf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of Accuracy Scores\n",
    "print(\"Accuracy scores computed using 10-fold cross-validation: {}\".format(cv_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.075Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accuracy Results - Mean and Standard Deviation\n",
    "show_scores(cv_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier - Data Set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Classifier - Random Forest\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_leaf_nodes=None, criterion='gini', max_depth=3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "rfc.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Classes\n",
    "y_pred = rfc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Probabilities\n",
    "y_pred_prob = rfc.predict_proba(X_test_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Predicted Probabilities\n",
    "y_pred_prob[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Accuracy\n",
    "rfc.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Accuracy\n",
    "rfc.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output of Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output of Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.145Z"
    }
   },
   "outputs": [],
   "source": [
    "# 10-fold Cross Validation including Stratification\n",
    "skf = StratifiedKFold(shuffle=True, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for AUC scores\n",
    "cv_auc = cross_val_score(rfc, predictors_scaled, target, cv=skf, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of AUC scores\n",
    "print(\"AUC scores computed using 10-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation for Accuracy Scores\n",
    "cv_acc = cross_val_score(rfc, predictors_scaled, target, cv=skf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print List of Accuracy Scores\n",
    "print(\"Accuracy scores computed using 10-fold cross-validation: {}\".format(cv_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy Results - Mean and Standard Deviation\n",
    "show_scores(cv_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Strategy - Data Set I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Strategy in Combination with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Lists To Store All Calculated Values For Every Variable of Every Run\n",
    "# Specified Period\n",
    "# Threshold 0.5\n",
    "spec_list_05_total = []\n",
    "spec_list_05_profit = []\n",
    "spec_list_05_loss = []\n",
    "spec_list_05_trans = []\n",
    "spec_list_05_acc = []\n",
    "spec_list_05_preds_corr = []\n",
    "spec_list_05_preds = []\n",
    "# Threshold 0.6\n",
    "spec_list_06_total = []\n",
    "spec_list_06_profit = []\n",
    "spec_list_06_loss = []\n",
    "spec_list_06_trans = []\n",
    "spec_list_06_acc = []\n",
    "spec_list_06_preds_corr = []\n",
    "spec_list_06_preds = []\n",
    "# Threshold 0.7\n",
    "spec_list_07_total = []\n",
    "spec_list_07_profit = []\n",
    "spec_list_07_loss = []\n",
    "spec_list_07_trans = []\n",
    "spec_list_07_acc = []\n",
    "spec_list_07_preds_corr = []\n",
    "spec_list_07_preds = []\n",
    "# Threshold 0.8\n",
    "spec_list_08_total = []\n",
    "spec_list_08_profit = []\n",
    "spec_list_08_loss = []\n",
    "spec_list_08_trans = []\n",
    "spec_list_08_acc = []\n",
    "spec_list_08_preds_corr = []\n",
    "spec_list_08_preds = []\n",
    "\n",
    "# Entire Period\n",
    "# Threshold 0.5\n",
    "list_05_total = []\n",
    "list_05_profit = []\n",
    "list_05_loss = []\n",
    "list_05_trans = []\n",
    "list_05_acc = []\n",
    "list_05_preds_corr = []\n",
    "list_05_preds = []\n",
    "# Threshold 0.6\n",
    "list_06_total = []\n",
    "list_06_profit = []\n",
    "list_06_loss = []\n",
    "list_06_trans = []\n",
    "list_06_acc = []\n",
    "list_06_preds_corr = []\n",
    "list_06_preds = []\n",
    "# Threshold 0.7\n",
    "list_07_total = []\n",
    "list_07_profit = []\n",
    "list_07_loss = []\n",
    "list_07_trans = []\n",
    "list_07_acc = []\n",
    "list_07_preds_corr = []\n",
    "list_07_preds = []\n",
    "# Threshold 0.8\n",
    "list_08_total = []\n",
    "list_08_profit = []\n",
    "list_08_loss = []\n",
    "list_08_trans = []\n",
    "list_08_acc = []\n",
    "list_08_preds_corr = []\n",
    "list_08_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.197Z"
    }
   },
   "outputs": [],
   "source": [
    "target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.203Z"
    }
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.208Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in df_dataset1.columns:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.214Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset1_copy[['Daily Return in USD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.229Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dataset1_copy[\"percentage_daily_return_bef_shift\"] = df_dataset1_copy[\"Daily Return in USD\"]/df_dataset1_copy[\"market/price_usd_close\"]\n",
    "df_dataset1_copy[\"percentage_daily_return_bef_shift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.239Z"
    }
   },
   "outputs": [],
   "source": [
    "# lreg = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
    "#               importance_type='gain', interaction_constraints='',\n",
    "#               learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
    "#               min_child_weight=1, monotone_constraints='()',\n",
    "#               n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
    "#               objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
    "#               reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "#               tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.246Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "orderedDict = collections.OrderedDict()\n",
    "\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.262Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "einsatz = 1000\n",
    "prestakepercentlist = []\n",
    "zinseszins = []\n",
    "profitpercent = []\n",
    "current_profitpercent = []\n",
    "spec_start_date = '2019-09-09'\n",
    "\n",
    "prestakepercentlist51 = []\n",
    "prestakepercentlist52 = []\n",
    "prestakepercentlist53 = []\n",
    "\n",
    "profitpercent51 = []\n",
    "profitpercent52 = []\n",
    "profitpercent53 = []\n",
    "\n",
    "current_profitpercent51 = []\n",
    "current_profitpercent52 = []\n",
    "current_profitpercent53 = []\n",
    "\n",
    "zinseszins51 = []\n",
    "zinseszins52 = []\n",
    "zinseszins53 = []\n",
    "\n",
    "current_zinseszins = []\n",
    "current_zinseszins51 = []\n",
    "current_zinseszins52 = []\n",
    "current_zinseszins53 = []\n",
    "\n",
    "\n",
    "# For Loop to Simulate K-fold Cross Validation\n",
    "for x in range(1, 200):\n",
    "\n",
    "    # Display Number of Runs\n",
    "    print(\"Run:\", x)\n",
    "\n",
    "    # Split Data into Training and Testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        predictors, target, test_size=0.3, stratify=target)\n",
    "\n",
    "    # Amount of Columns\n",
    "    n_nodes = predictors.shape[1]\n",
    "\n",
    "    # Instantiation of MinMaxScaler\n",
    "    norm_scaler = MinMaxScaler()\n",
    "\n",
    "    # Training and Application of Feature Scaler on Training Data of Predictors\n",
    "    X_train_scaled = pd.DataFrame(norm_scaler.fit_transform(X_train))\n",
    "    X_train_scaled = X_train\n",
    "\n",
    "    # Application of Feature Scaler on Test Data of Predictors\n",
    "    X_test_scaled = norm_scaler.transform(X_test)\n",
    "    X_test_scaled = X_test\n",
    "    \n",
    "\n",
    "    # Instantiate Classifier Logistic Regression\n",
    "    lreg = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
    "              min_child_weight=1, monotone_constraints='()',\n",
    "              n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
    "              objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
    "              reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "    # Train Classifier\n",
    "    lreg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prediction of Classes\n",
    "    y_pred = lreg.predict(X_test_scaled)\n",
    "\n",
    "    # Display Confusion Matrix and Classification Report\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Prediction of Probabilities\n",
    "    y_pred_prob = lreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Create Data Frame of Actual Values - y_test\n",
    "    df_y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    # Add Predicted Probabilities to Data Frame\n",
    "    df_y_test[\"y_pred_prob\"] = y_pred_prob\n",
    "\n",
    "    # Add Predicted Classes (0 or 1) to Data Frame to Cross-Check Predicted Probabilities\n",
    "    df_y_test[\"y_pred\"] = y_pred\n",
    "\n",
    "    # Select Actual Daily Returns in USD from the Data Set Copy\n",
    "    df_dataset1_copy_dailyreturns = df_dataset1_copy[['Daily Return in USD']]\n",
    "\n",
    "    # Merge Predictions and Daily Returns Data Frame on the Column \"Date\"\n",
    "    df_y_test_withdailyreturns = pd.merge(\n",
    "        df_y_test, df_dataset1_copy_dailyreturns, on='Date')\n",
    "\n",
    "    df_dataset1_copy_percentage = df_dataset1_copy[['percentage_daily_return_bef_shift']]\n",
    "    df_y_test_withdailyreturns = pd.merge(\n",
    "        df_y_test_withdailyreturns, df_dataset1_copy_percentage, on='Date')\n",
    "\n",
    "    # Add Bitcoin Price in USD\n",
    "    df_y_test_withdailyreturns = pd.merge(\n",
    "        df_y_test_withdailyreturns, df_dataset1_copy['market/price_usd_close'], on='Date')\n",
    "\n",
    "    df_y_test_withdailyreturns['y_pred_prob_bereinigt'] = np.where(\n",
    "        df_y_test_withdailyreturns['y_pred_prob'] < 0.5, 1-df_y_test_withdailyreturns['y_pred_prob'], df_y_test_withdailyreturns['y_pred_prob'])\n",
    "\n",
    "    df_y_test_withdailyreturns[\"Einsatz\"] = einsatz #* \\\n",
    "        #df_y_test_withdailyreturns[\"y_pred_prob_bereinigt\"]  # **2\n",
    "\n",
    "    # Add and Calculate Transaction Costs based on the Bitcoin Price in USD\n",
    "    df_y_test_withdailyreturns['Transaction Costs in USD'] = df_y_test_withdailyreturns['Einsatz']*0.00027*2\n",
    "\n",
    "    # Create List of Multiple Thresholds\n",
    "    # A Threshold of 0.7 means that only predicted probabilites with more than 0.7 or less than 0.3 are considered further\n",
    "    list_thresholdcalc = [0.5, 0.51, 0.52, 0.53]\n",
    "\n",
    "    # Try and Except Block to Address Cases in Which a Data Frame does NOT Contain Any Values Because No predicted Probability Meets the Threshold Requirements\n",
    "    try:\n",
    "\n",
    "        # For Loop Over List\n",
    "        for thresholdcalc in list_thresholdcalc:\n",
    "\n",
    "            # Creation of a Data Frame Containing Only Probability Predictions Which Meet the Threshold Requirements\n",
    "            df_y_test_probaall = df_y_test.loc[(df_y_test['y_pred_prob'] > thresholdcalc) | (\n",
    "                df_y_test['y_pred_prob'] < 1-thresholdcalc)]\n",
    "\n",
    "            # Calculation Of Accuracy Only for the Data Subset Meeting Threshold Requirements\n",
    "            acc_calc = len(df_y_test_probaall[df_y_test_probaall['Class']\n",
    "                                              == df_y_test_probaall['y_pred']]) / len(df_y_test_probaall)\n",
    "\n",
    "            print(\"Threshold of:\", thresholdcalc)\n",
    "            print(classification_report(\n",
    "                df_y_test_probaall[\"Class\"], df_y_test_probaall[\"y_pred\"]))\n",
    "\n",
    "            # Creation of Another Data Frame including Returns and Transaction Costs\n",
    "            # Only For Probability Predicitons that Meet the Threshold Requirements\n",
    "            df_y_test_probaall_withdailyreturns = df_y_test_withdailyreturns.loc[(\n",
    "                df_y_test_withdailyreturns['y_pred_prob'] > thresholdcalc) | (df_y_test_withdailyreturns['y_pred_prob'] < 1-thresholdcalc)]\n",
    "\n",
    "            # Trading Assumption: Bet Amount = 1 Bitcoin for Each Prediction\n",
    "            # Select Daily Returns Only From True Predictions Where Predicted Class equals Actual Class.\n",
    "            # It does not matter whether Daily Returns are Positive or Negative as Short-Selling Option Allows To Profit From Falling Prices\n",
    "            profit = df_y_test_probaall_withdailyreturns[\"percentage_daily_return_bef_shift\"][\n",
    "                df_y_test_probaall_withdailyreturns['Class'] == df_y_test_probaall_withdailyreturns['y_pred']].abs().sum()\n",
    "\n",
    "            # Select  Daily Returns Only From False Predictions Where Predicted Class equals Actual Class\n",
    "            loss = df_y_test_probaall_withdailyreturns[\"percentage_daily_return_bef_shift\"][df_y_test_probaall_withdailyreturns['Class']\n",
    "                                                                                  != df_y_test_probaall_withdailyreturns['y_pred']].abs().sum()\n",
    "\n",
    "            print(\"prestake profit-loss {:.2f} x\".format(profit-loss))\n",
    "\n",
    "            if thresholdcalc == 0.5:\n",
    "                prestakepercentlist.append(int((profit-loss)*100))\n",
    "            if thresholdcalc == 0.51:\n",
    "                prestakepercentlist51.append(int((profit-loss)*100))\n",
    "            if thresholdcalc == 0.52:\n",
    "                prestakepercentlist52.append(int((profit-loss)*100))\n",
    "            if thresholdcalc == 0.53:\n",
    "                prestakepercentlist53.append(int((profit-loss)*100))\n",
    "\n",
    "            loss = loss * einsatz\n",
    "            profit = profit * einsatz\n",
    "\n",
    "            # Calculation of Transaction Costs based On BTC Value in USD. Each Future Trade considers Transaction Costs twice, becauase of Buying and Selling the Future\n",
    "            transaction_costs = df_y_test_probaall_withdailyreturns['Transaction Costs in USD'].sum(\n",
    "            )\n",
    "\n",
    "            # Calculation of Total Profit Considering Profit, Loss and Transaction Costs\n",
    "            total = profit-loss-transaction_costs\n",
    "\n",
    "            # In addition, the same setting with different time horizon\n",
    "            currentpreds_all = df_y_test_probaall_withdailyreturns[\n",
    "                df_y_test_probaall_withdailyreturns.index >= spec_start_date]\n",
    "\n",
    "            # the len\n",
    "            currentpreds_all_count = len(currentpreds_all)\n",
    "\n",
    "            # Analog to Profit Calculation Above\n",
    "            currentpreds_winning_total = currentpreds_all[\"percentage_daily_return_bef_shift\"][(\n",
    "                currentpreds_all['Class'] == currentpreds_all['y_pred'])].abs().sum()\n",
    "            # Analog to Loss Calculation Above\n",
    "            currentpreds_loosing_total = currentpreds_all[\"percentage_daily_return_bef_shift\"][(\n",
    "                currentpreds_all['Class'] != currentpreds_all['y_pred'])].abs().sum()\n",
    "            # Analog to Calculation of Transaction Costs Above\n",
    "            currentpreds_transaction_costs = currentpreds_all['Transaction Costs in USD'][(\n",
    "                currentpreds_all['Class'] != currentpreds_all['y_pred'])].abs().sum()\n",
    "\n",
    "            currentpreds_loosing_total = currentpreds_loosing_total * einsatz\n",
    "            currentpreds_winning_total = currentpreds_winning_total * einsatz\n",
    "\n",
    "            # Analog to Calculation of Total Profit\n",
    "            currentpreds_total = currentpreds_winning_total - \\\n",
    "                currentpreds_loosing_total-currentpreds_transaction_costs\n",
    "            \n",
    "            print(\"currentpreds_winning_total {} - currentpreds_loosing_total {} -currentpreds_transaction_costs {} \".format(\n",
    "            currentpreds_winning_total, currentpreds_loosing_total, currentpreds_transaction_costs))\n",
    "\n",
    "            # Condition\n",
    "            # In Case the Shorter Time Horizon Does Not Include any Sample -> Empyt Output\n",
    "            if currentpreds_all_count == 0:\n",
    "                currentpreds_all_count = '-'\n",
    "                current_acc_calc = '-'\n",
    "\n",
    "            # Otherwise Calculation of Accuracy analog to theeins Accuracy Calculation for the Entire Period\n",
    "            else:\n",
    "                current_acc_calc = (len(currentpreds_all[\"Daily Return in USD\"][(\n",
    "                    currentpreds_all['Class'] == currentpreds_all['y_pred'])]))/currentpreds_all_count\n",
    "\n",
    "            # Display of Threshold, Total Profit, Profit, Loss, Transaction Costs, Accuracy and Correct Number of Predictions Both for the Specified Time Horizon and the Entire Time\n",
    "            print(\"SpecPeriod:\")\n",
    "            print(\"einsatz\", int(currentpreds_all[\"Einsatz\"].sum()), \"Total Return in SpecPeriod:\", int(currentpreds_total), \"/ Profit: \", int(currentpreds_winning_total), \"/ Loss: \", int(currentpreds_loosing_total), \"/ TransCosts: \", int(currentpreds_transaction_costs),\n",
    "                  \"/ #CorrectPreds:\", (len(currentpreds_all[\"Daily Return in USD\"][(currentpreds_all['Class'] == currentpreds_all['y_pred'])])), \"of\", len(currentpreds_all))\n",
    "\n",
    "            print(\"CalcAccuracy {:.2f} -- profit in %/bet: {:.2f}\".format(current_acc_calc, 100*(int(\n",
    "                currentpreds_total) / len(currentpreds_all)) / (currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all))))\n",
    "            print(\"einsatz * 1+profit% ^ # wetten {:.2f}\".format(einsatz*(1+(int(currentpreds_total) / len(currentpreds_all)) / (\n",
    "                currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all))) ** len(currentpreds_all)))\n",
    "\n",
    "            if thresholdcalc == 0.5:\n",
    "                current_profitpercent.append(100*(int(currentpreds_total) / len(\n",
    "                    currentpreds_all)) / (currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all)))\n",
    "            if thresholdcalc == 0.51:\n",
    "                current_profitpercent51.append(100*(int(currentpreds_total) / len(\n",
    "                    currentpreds_all)) / (currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all)))\n",
    "            if thresholdcalc == 0.52:\n",
    "                current_profitpercent52.append(100*(int(currentpreds_total) / len(\n",
    "                    currentpreds_all)) / (currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all)))\n",
    "            if thresholdcalc == 0.53:\n",
    "                current_profitpercent53.append(100*(int(currentpreds_total) / len(\n",
    "                    currentpreds_all)) / (currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all)))\n",
    "\n",
    "            print()\n",
    "            print(\"Entire Period:\")\n",
    "            print(\"einsatz\", int(df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum()), \"TotReturn:\", int(total), \"/ Profit: \", int(profit), \"/ Loss: \", int(loss), \"/ TransCosts: \", int(\n",
    "                transaction_costs), \"/ CalcAccuracy\", round(acc_calc, 2), \"/ #CorrectPreds:\", len(df_y_test_probaall[df_y_test_probaall['Class'] == df_y_test_probaall['y_pred']]),  \"Of\", len(df_y_test_probaall))\n",
    "            print(\"einsatz/bet\", df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall), \"TotReturn / bet\", int(total) / len(\n",
    "                df_y_test_probaall), \"/ TranCosts / bet \", int(transaction_costs) / len(df_y_test_probaall), \"/ CalcuAcc\", round(acc_calc, 2))\n",
    "            print(\"profit in %/bet: {:.2f}\".format(100*(int(total) / len(df_y_test_probaall)) / (\n",
    "                df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall))))\n",
    "\n",
    "            if thresholdcalc == 0.5:\n",
    "                profitpercent.append(100*(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall)))\n",
    "            if thresholdcalc == 0.51:\n",
    "                profitpercent51.append(100*(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall)))\n",
    "            if thresholdcalc == 0.52:\n",
    "                profitpercent52.append(100*(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall)))\n",
    "            if thresholdcalc == 0.53:\n",
    "                profitpercent53.append(100*(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall)))\n",
    "\n",
    "            print(\"einsatz * 1+profit% ^ # wetten {:.2f}\".format(einsatz*(1+(int(total) / len(df_y_test_probaall)) / (\n",
    "                df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall))) ** len(df_y_test_probaall)))\n",
    "            if thresholdcalc == 0.5:\n",
    "                zinseszins.append(einsatz*(1+(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall))) ** len(df_y_test_probaall))\n",
    "\n",
    "                current_zinseszins.append(einsatz*(1+(int(currentpreds_total) / len(currentpreds_all)) / (\n",
    "                    currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all))) ** len(currentpreds_all))\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "            # Store All Calculated Values For Every Variable of Every Run\n",
    "            # Specified Period\n",
    "            # Threshold = 0.5\n",
    "            if thresholdcalc == 0.5:\n",
    "                spec_list_05_total.append(currentpreds_total)\n",
    "                spec_list_05_profit.append(currentpreds_winning_total)\n",
    "                spec_list_05_loss.append(currentpreds_loosing_total)\n",
    "                spec_list_05_trans.append(currentpreds_transaction_costs)\n",
    "                spec_list_05_acc.append(current_acc_calc)\n",
    "                spec_list_05_preds_corr.append((len(df_y_test_probaall_withdailyreturns[\"Daily Return in USD\"][(\n",
    "                    df_y_test_probaall_withdailyreturns.index >= spec_start_date) & (df_y_test_probaall_withdailyreturns['Class'] == df_y_test_probaall_withdailyreturns['y_pred'])])))\n",
    "                spec_list_05_preds.append(currentpreds_all_count)\n",
    "                # Entire Period:\n",
    "                list_05_total.append(total)\n",
    "                list_05_profit.append(profit)\n",
    "                list_05_loss.append(loss)\n",
    "                list_05_trans.append(transaction_costs)\n",
    "                list_05_acc.append(acc_calc)\n",
    "                list_05_preds_corr.append(len(\n",
    "                    df_y_test_probaall[df_y_test_probaall['Class'] == df_y_test_probaall['y_pred']]))\n",
    "                list_05_preds.append(len(df_y_test_probaall))\n",
    "\n",
    "            # Threshold = 0.6\n",
    "            if thresholdcalc == 0.51:\n",
    "                spec_list_06_total.append(currentpreds_total)\n",
    "                spec_list_06_profit.append(currentpreds_winning_total)\n",
    "                spec_list_06_loss.append(currentpreds_loosing_total)\n",
    "                spec_list_06_trans.append(currentpreds_transaction_costs)\n",
    "                spec_list_06_acc.append(current_acc_calc)\n",
    "                spec_list_06_preds_corr.append((len(df_y_test_probaall_withdailyreturns[\"Daily Return in USD\"][(\n",
    "                    df_y_test_probaall_withdailyreturns.index >= spec_start_date) & (df_y_test_probaall_withdailyreturns['Class'] == df_y_test_probaall_withdailyreturns['y_pred'])])))\n",
    "                spec_list_06_preds.append(currentpreds_all_count)\n",
    "                # Entire Period:\n",
    "                list_06_total.append(total)\n",
    "                list_06_profit.append(profit)\n",
    "                list_06_loss.append(loss)\n",
    "                list_06_trans.append(transaction_costs)\n",
    "                list_06_acc.append(acc_calc)\n",
    "                list_06_preds_corr.append(len(\n",
    "                    df_y_test_probaall[df_y_test_probaall['Class'] == df_y_test_probaall['y_pred']]))\n",
    "                list_06_preds.append(len(df_y_test_probaall))\n",
    "\n",
    "                zinseszins51.append(einsatz*(1+(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall))) ** len(df_y_test_probaall))\n",
    "\n",
    "                current_zinseszins51.append(einsatz*(1+(int(currentpreds_total) / len(currentpreds_all)) / (\n",
    "                    currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all))) ** len(currentpreds_all))\n",
    "\n",
    "            # Threshold = 0.7\n",
    "            if thresholdcalc == 0.52:\n",
    "                spec_list_07_total.append(currentpreds_total)\n",
    "                spec_list_07_profit.append(currentpreds_winning_total)\n",
    "                spec_list_07_loss.append(currentpreds_loosing_total)\n",
    "                spec_list_07_trans.append(currentpreds_transaction_costs)\n",
    "                spec_list_07_acc.append(current_acc_calc)\n",
    "                spec_list_07_preds_corr.append((len(df_y_test_probaall_withdailyreturns[\"Daily Return in USD\"][(\n",
    "                    df_y_test_probaall_withdailyreturns.index >= spec_start_date) & (df_y_test_probaall_withdailyreturns['Class'] == df_y_test_probaall_withdailyreturns['y_pred'])])))\n",
    "                spec_list_07_preds.append(currentpreds_all_count)\n",
    "                # Entire Period:\n",
    "                list_07_total.append(total)\n",
    "                list_07_profit.append(profit)\n",
    "                list_07_loss.append(loss)\n",
    "                list_07_trans.append(transaction_costs)\n",
    "                list_07_acc.append(acc_calc)\n",
    "                list_07_preds_corr.append(len(\n",
    "                    df_y_test_probaall[df_y_test_probaall['Class'] == df_y_test_probaall['y_pred']]))\n",
    "                list_07_preds.append(len(df_y_test_probaall))\n",
    "                zinseszins52.append(einsatz*(1+(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall))) ** len(df_y_test_probaall))\n",
    "\n",
    "                current_zinseszins52.append(einsatz*(1+(int(currentpreds_total) / len(currentpreds_all)) / (\n",
    "                    currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all))) ** len(currentpreds_all))\n",
    "\n",
    "            # Threshold = 0.8\n",
    "            if thresholdcalc == 0.53:\n",
    "                spec_list_08_total.append(currentpreds_total)\n",
    "                spec_list_08_profit.append(currentpreds_winning_total)\n",
    "                spec_list_08_loss.append(currentpreds_loosing_total)\n",
    "                spec_list_08_trans.append(currentpreds_transaction_costs)\n",
    "                spec_list_08_acc.append(current_acc_calc)\n",
    "                spec_list_08_preds_corr.append((len(df_y_test_probaall_withdailyreturns[\"Daily Return in USD\"][(\n",
    "                    df_y_test_probaall_withdailyreturns.index >= spec_start_date) & (df_y_test_probaall_withdailyreturns['Class'] == df_y_test_probaall_withdailyreturns['y_pred'])])))\n",
    "                spec_list_08_preds.append(currentpreds_all_count)\n",
    "                # Entire Period:\n",
    "                list_08_total.append(total)\n",
    "                list_08_profit.append(profit)\n",
    "                list_08_loss.append(loss)\n",
    "                list_08_trans.append(transaction_costs)\n",
    "                list_08_acc.append(acc_calc)\n",
    "                list_08_preds_corr.append(len(\n",
    "                    df_y_test_probaall[df_y_test_probaall['Class'] == df_y_test_probaall['y_pred']]))\n",
    "                list_08_preds.append(len(df_y_test_probaall))\n",
    "                zinseszins53.append(einsatz*(1+(int(total) / len(df_y_test_probaall)) / (\n",
    "                    df_y_test_probaall_withdailyreturns[\"Einsatz\"].sum() / len(df_y_test_probaall))) ** len(df_y_test_probaall))\n",
    "\n",
    "                current_zinseszins53.append(einsatz*(1+(int(currentpreds_total) / len(currentpreds_all)) / (\n",
    "                    currentpreds_all[\"Einsatz\"].sum() / len(currentpreds_all))) ** len(currentpreds_all))\n",
    "\n",
    "    except:\n",
    "        print(\"error\")\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "accuarylist = []\n",
    "\n",
    "for x in range(30):\n",
    "    # Split Data into Training and Testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        predictors, target, test_size=0.3, stratify=target)\n",
    "    \n",
    "    # Scaler Training and Transformation of Training Set\n",
    "    #X_train_scaled = pd.DataFrame(norm_scaler.fit_transform(X_train))\n",
    "    #X_test_scaled = pd.DataFrame(norm_scaler.fit_transform(X_test))\n",
    "    \n",
    "    # Train Classifier\n",
    "    lreg.fit(X_train, y_train)\n",
    "    y_pred = lreg.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuarylist.append(accuracy * 100.0)\n",
    "\n",
    "mean_auc_10 = np.mean(accuarylist)\n",
    "mean_auc_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.287Z"
    }
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "result_to_csv = []\n",
    "\n",
    "result_to_csv.append((\"statistics.median(prestakepercentlist)\", statistics.median(prestakepercentlist) ))\n",
    "result_to_csv.append((\"statistics.median(profitpercent)\", statistics.median(profitpercent) ))\n",
    "result_to_csv.append((\"statistics.median(zinseszins)\", statistics.median(zinseszins) ))\n",
    "result_to_csv.append((\"statistics.median(current_profitpercent)\", statistics.median(current_profitpercent) ))\n",
    "result_to_csv.append((\"statistics.median(current_zinseszins)\", statistics.median(current_zinseszins) ))\n",
    "result_to_csv.append((\"statistics.median(prestakepercentlist51)\", statistics.median(prestakepercentlist51) ))\n",
    "result_to_csv.append((\"statistics.median(profitpercent51)\", statistics.median(profitpercent51) ))\n",
    "result_to_csv.append((\"statistics.median(zinseszins51)\", statistics.median(zinseszins51) ))\n",
    "result_to_csv.append((\"statistics.median(current_profitpercent51)\", statistics.median(current_profitpercent51) ))\n",
    "result_to_csv.append((\"statistics.median(current_zinseszins51)\", statistics.median(current_zinseszins51) ))\n",
    "result_to_csv.append((\"statistics.median(prestakepercentlist52)\", statistics.median(prestakepercentlist52) ))\n",
    "result_to_csv.append((\"statistics.median(profitpercent52)\", statistics.median(profitpercent52) ))\n",
    "result_to_csv.append((\"statistics.median(zinseszins52)\", statistics.median(zinseszins52) ))\n",
    "result_to_csv.append((\"statistics.median(current_profitpercent52)\", statistics.median(current_profitpercent52) ))\n",
    "result_to_csv.append((\"statistics.median(current_zinseszins52)\", statistics.median(current_zinseszins52) ))\n",
    "result_to_csv.append((\"statistics.median(prestakepercentlist53)\", statistics.median(prestakepercentlist53) ))\n",
    "result_to_csv.append((\"statistics.median(profitpercent53)\", statistics.median(profitpercent53) ))\n",
    "result_to_csv.append((\"statistics.median(zinseszins53)\", statistics.median(zinseszins53) ))\n",
    "result_to_csv.append((\"statistics.median(current_profitpercent53)\", statistics.median(current_profitpercent53) ))\n",
    "result_to_csv.append((\"statistics.median(current_zinseszins53)\", statistics.median(current_zinseszins53) ))\n",
    "df_zusammen = pd.DataFrame(result_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.294Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "df_zusammen[\"algo\"] = pd.Series([lreg]) \n",
    "df_zusammen[\"spec_start_date\"] = pd.Series([spec_start_date]) \n",
    "df_zusammen[\"X_train.shape\"] = pd.Series([X_train.shape]) \n",
    "df_zusammen[\"X_test.shape\"] = pd.Series([X_test.shape]) \n",
    "df_zusammen[\"mean_auc_10\"] = pd.Series([mean_auc_10]) \n",
    "df_zusammen[\"df_dataset1.columns\"] = pd.Series([df_dataset1.columns.tolist()])\n",
    "\n",
    "out_path = \"{}_{}_ppd{}.xlsx\".format(int(time.time()), str(lreg)[:10], str(round( statistics.median(current_profitpercent), 2)) )\n",
    "\n",
    "writer = pd.ExcelWriter(out_path , engine='xlsxwriter')\n",
    "df_zusammen.to_excel(writer, sheet_name='Tabelle1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.309Z"
    }
   },
   "outputs": [],
   "source": [
    "print( statistics.median(prestakepercentlist) )\n",
    "print( statistics.median(profitpercent) )\n",
    "print( statistics.median(zinseszins) )\n",
    "print( statistics.median(current_profitpercent) )\n",
    "print( statistics.median(current_zinseszins) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.316Z"
    }
   },
   "outputs": [],
   "source": [
    "print( statistics.median(prestakepercentlist51) )\n",
    "print( statistics.median(profitpercent51) )\n",
    "print( statistics.median(zinseszins51) )\n",
    "print( statistics.median(current_profitpercent51) )\n",
    "print( statistics.median(current_zinseszins51) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.325Z"
    }
   },
   "outputs": [],
   "source": [
    "print( statistics.median(prestakepercentlist52) )\n",
    "print( statistics.median(profitpercent52) )\n",
    "print( statistics.median(zinseszins52) )\n",
    "print( statistics.median(current_profitpercent52) )\n",
    "print( statistics.median(current_zinseszins52) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.332Z"
    }
   },
   "outputs": [],
   "source": [
    "print( statistics.median(prestakepercentlist53) )\n",
    "print( statistics.median(profitpercent53) )\n",
    "print( statistics.median(zinseszins53) )\n",
    "print( statistics.median(current_profitpercent53) )\n",
    "print( statistics.median(current_zinseszins53) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.340Z"
    }
   },
   "outputs": [],
   "source": [
    "current_profitpercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.346Z"
    }
   },
   "outputs": [],
   "source": [
    "currentpreds_all[\"Daily Return in USD\"][(\n",
    "    currentpreds_all['Class'] == currentpreds_all['y_pred'])].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.353Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "currentpreds_all[\"Daily Return in USD\"][(\n",
    "    currentpreds_all['Class'] != currentpreds_all['y_pred'])].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepdive in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.365Z"
    }
   },
   "outputs": [],
   "source": [
    "current_profitpercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.373Z"
    }
   },
   "outputs": [],
   "source": [
    "profitpercent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.380Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test_withdailyreturns.sort_values(\"Daily Return in USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.387Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test_withdailyreturns[df_y_test_withdailyreturns[\"Class\"] != df_y_test_withdailyreturns[\"y_pred\"]].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.394Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test_withdailyreturns[df_y_test_withdailyreturns[\"Class\"] == df_y_test_withdailyreturns[\"y_pred\"]].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.400Z"
    }
   },
   "outputs": [],
   "source": [
    "dfrights = df_y_test_withdailyreturns[df_y_test_withdailyreturns[\"Class\"] == df_y_test_withdailyreturns[\"y_pred\"]]\n",
    "dfwrongs = df_y_test_withdailyreturns[df_y_test_withdailyreturns[\"Class\"] != df_y_test_withdailyreturns[\"y_pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.408Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test_withdailyreturns = df_y_test_withdailyreturns.sort_index()\n",
    "df_y_test_withdailyreturns[\"dfrights\"] = np.where(df_y_test_withdailyreturns[\"Class\"] == df_y_test_withdailyreturns[\"y_pred\"], 1.0, 0.0)\n",
    "df_y_test_withdailyreturns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.414Z"
    }
   },
   "outputs": [],
   "source": [
    "df_y_test_withdailyreturns['gain'] = np.where(df_y_test_withdailyreturns[\"dfrights\"]==1.0, df_y_test_withdailyreturns[\"percentage_daily_return_bef_shift\"].abs(), -1*df_y_test_withdailyreturns[\"percentage_daily_return_bef_shift\"].abs() )\n",
    "df_y_test_withdailyreturns['gain_cumsum'] = df_y_test_withdailyreturns['gain'].cumsum()\n",
    "df_y_test_withdailyreturns['gain_cumsum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.422Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df_y_test_withdailyreturns, y=df_y_test_withdailyreturns['gain_cumsum'], title='Algo gains over time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.437Z"
    }
   },
   "outputs": [],
   "source": [
    "dfwrongs.abs().index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.446Z"
    }
   },
   "outputs": [],
   "source": [
    "dfwrongs.abs().percentage_daily_return_bef_shift.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.456Z"
    }
   },
   "outputs": [],
   "source": [
    "dfwrongs = dfwrongs.sort_index()\n",
    "dfwrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.467Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# show perecentages of wrong predictions over time\n",
    "fig = go.Figure(data=go.Scatter(x=dfwrongs.abs().index, y=dfwrongs.abs().percentage_daily_return_bef_shift.values))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.476Z"
    }
   },
   "outputs": [],
   "source": [
    "# show perecentages of wrong predictions histogramm\n",
    "\n",
    "dfwrongs.abs().sort_values(\"percentage_daily_return_bef_shift\")\n",
    "fig = px.histogram(dfwrongs.abs(), x=\"percentage_daily_return_bef_shift\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.483Z"
    }
   },
   "outputs": [],
   "source": [
    "dfrights.abs().sort_values(\"percentage_daily_return_bef_shift\")\n",
    "fig = px.histogram(dfrights.abs(), x=\"percentage_daily_return_bef_shift\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.492Z"
    }
   },
   "outputs": [],
   "source": [
    "dfwrongs[\"year\"] = dfwrongs.index.year\n",
    "dfwrongs.abs().groupby(['year']).agg(['sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.500Z"
    }
   },
   "outputs": [],
   "source": [
    "# HERE DIFF LOOK AT percentage_daily_return_bef_shift\n",
    "\n",
    "dfrights[\"year\"] = dfrights.index.year\n",
    "dfrights.abs().groupby(['year']).agg(['sum', 'count']) - dfwrongs.abs().groupby(['year']).agg(['sum', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict on tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split Data into Training and Testing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, stratify=target)\n",
    "\n",
    "# Amount of Columns\n",
    "n_nodes = predictors.shape[1]\n",
    "print(\"Amount of Columns\", n_nodes)\n",
    "\n",
    "# Instantiation of MinMaxScaler\n",
    "norm_scaler = MinMaxScaler()\n",
    "\n",
    "# Training and Application of Feature Scaler on Training Data of Predictors\n",
    "predictors_scaled = pd.DataFrame(norm_scaler.fit_transform(predictors))\n",
    "\n",
    "# Instantiate Classifier Logistic Regression\n",
    "#lreg = LogisticRegression(max_iter=12000) \n",
    "\n",
    "# Train Classifier\n",
    "lreg.fit(predictors_scaled, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get x train data of today to predict tomorrow y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.519Z"
    }
   },
   "outputs": [],
   "source": [
    "most_current_day = df_dataset1.tail(1)\n",
    "dateofprediction = most_current_day.index\n",
    "most_current_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.528Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Setup of Predictors (Indpendent Variables) by Excluding Target Variable\n",
    "    most_current_day_predictors = most_current_day.drop(['Class'], axis=1).values.astype(np.float32)\n",
    "except:\n",
    "    # Setup of Predictors (Indpendent Variables) by Excluding Target Variable\n",
    "    most_current_day_predictors = most_current_day.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.538Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaler Transformation of Entire Training Set - Input for Cross-Validation\n",
    "most_current_day_predictors_scaled = norm_scaler.transform(most_current_day_predictors)\n",
    "most_current_day_predictors_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.547Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Probabilities\n",
    "most_current_day_pred_prob = lreg.predict_proba(most_current_day_predictors_scaled)[:,1]\n",
    "most_current_day_pred_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction of Classes\n",
    "most_current_day_pred = lreg.predict(most_current_day_predictors_scaled)\n",
    "algoprediction = int( most_current_day_pred[0] )\n",
    "algoprediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-27T11:28:52.561Z"
    }
   },
   "outputs": [],
   "source": [
    "dateofprediction[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
